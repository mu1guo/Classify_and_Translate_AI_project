{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1、导入相关包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T05:27:24.351652Z",
     "iopub.status.busy": "2022-07-20T05:27:24.350929Z",
     "iopub.status.idle": "2022-07-20T05:27:26.428062Z",
     "shell.execute_reply": "2022-07-20T05:27:26.426855Z",
     "shell.execute_reply.started": "2022-07-20T05:27:24.351608Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "# 数据科学包\n",
    "import random                      # 随机切分数据集\n",
    "import numpy as np                 # 常用数据科学包\n",
    "import pandas as pd              # 图像读取\n",
    "import matplotlib.pyplot as plt    # 代码中快速验证\n",
    "import cv2                         # 图像包\n",
    "\n",
    "# 深度学习包\n",
    "import paddle.vision.transforms as tf      # 数据增强\n",
    "from paddle.io import Dataset, DataLoader  # 定义数据集\n",
    "import paddle.nn as nn                     # 网络\n",
    "\n",
    "\n",
    "#引入其他必要的工具库\n",
    "import time\n",
    "from functools import partial\n",
    "from pprint import pprint\n",
    "import yaml\n",
    "import logging\n",
    "import argparse\n",
    "from attrdict import AttrDict\n",
    "import numpy as np\n",
    "import paddle\n",
    "import paddle.nn as nn\n",
    "from paddle.nn import TransformerDecoder,TransformerDecoderLayer,TransformerEncoder,TransformerEncoderLayer\n",
    "from paddle.nn import functional as F\n",
    "import paddle.distributed as dist\n",
    "from paddle.io import DataLoader,BatchSampler\n",
    "from paddlenlp.data import Vocab, Pad\n",
    "from paddlenlp.datasets import load_dataset\n",
    "from paddlenlp.transformers import TransformerModel, InferTransformerModel, CrossEntropyCriterion, position_encoding_init\n",
    "from paddlenlp.utils.log import logger\n",
    "\n",
    "from helper.utils import post_process_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、准备网络模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "搭建网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义CNN模型：随机嵌入\n",
    "class ModifiedCNN(nn.Layer):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, kernel_size, output_size, maxlength):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.cnn = nn.Conv1D(embedding_dim, hidden_dim, kernel_size)\n",
    "        self.maxpool = nn.MaxPool1D(maxlength - kernel_size + 1)\n",
    "        self.bi_lstm = nn.LSTM(hidden_dim, hidden_dim // 2, num_layers=1, direction='bidirectional')\n",
    "        self.dense = nn.Sequential(nn.Dropout(0.3), nn.Linear(hidden_dim, output_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        embed_x = self.embed(x)\n",
    "        cnn_x = self.cnn(embed_x.transpose((0, 2, 1)))\n",
    "        pool_x = self.maxpool(cnn_x)\n",
    "        lstm_out, _ = self.bi_lstm(pool_x.squeeze(-1).transpose((0, 2, 1)))  # Transpose input for PaddlePaddle LSTM\n",
    "        out = self.dense(lstm_out[:, -1, :])  # Take the output of the last time step\n",
    "        return out\n",
    "\n",
    "\n",
    "# 定义模型\n",
    "vocab_size = 10000 # 词汇数量\n",
    "embedding_dim = 1024 # 词嵌入维度\n",
    "hidden_dim = 128 # 隐藏层维度，也就是CNN网络层卷积核的个数\n",
    "kernel_size = 3 # 卷积核大小\n",
    "output_size = 14  # 分类的类别数\n",
    "maxlength = 30  # 新闻标题的最大长度\n",
    "\n",
    "model = ModifiedCNN(vocab_size, embedding_dim, hidden_dim, kernel_size, output_size, maxlength)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModifiedCNN(\n",
      "  (embed): Embedding(10000, 1024, padding_idx=0, sparse=False)\n",
      "  (cnn): Conv1D(1024, 128, kernel_size=[3], data_format=NCL)\n",
      "  (maxpool): MaxPool1D(kernel_size=28, stride=None, padding=0)\n",
      "  (bi_lstm): LSTM(128, 64\n",
      "    (0): BiRNN(\n",
      "      (cell_fw): LSTMCell(128, 64)\n",
      "      (cell_bw): LSTMCell(128, 64)\n",
      "    )\n",
      "  )\n",
      "  (dense): Sequential(\n",
      "    (0): Dropout(p=0.3, axis=None, mode=upscale_in_train)\n",
      "    (1): Linear(in_features=128, out_features=14, dtype=float32)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 输出模型结构\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3、数据集准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jieba tokenize...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.966 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "source learn-bpe and apply-bpe...\n",
      "no pair has frequency >= 2. Stopping\n",
      "target learn-bpe and apply-bpe...\n",
      "no pair has frequency >= 2. Stopping\n",
      "source get-vocab. if loading pretrained model, use its vocab.\n",
      "target get-vocab. if loading pretrained model, use its vocab.\n",
      "Over.\n"
     ]
    }
   ],
   "source": [
    "# 数据预处理过程，包括jieba分词、bpe分词和词表。\n",
    "!bash ./helper/preprocess.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义读取本地数据的方法\n",
    "def read(src_path, tgt_path, is_predict=False):\n",
    "    if is_predict:\n",
    "        with open(src_path, 'r', encoding='utf8') as src_f:\n",
    "            for src_line in src_f.readlines():\n",
    "                src_line = src_line.strip()\n",
    "                if not src_line:\n",
    "                    continue\n",
    "                yield {'src':src_line, 'tgt':''}\n",
    "    else:\n",
    "        with open(src_path, 'r', encoding='utf8') as src_f, open(tgt_path, 'r', encoding='utf8') as tgt_f:\n",
    "            for src_line, tgt_line in zip(src_f.readlines(), tgt_f.readlines()):\n",
    "                src_line = src_line.strip()\n",
    "                if not src_line:\n",
    "                    continue\n",
    "                tgt_line = tgt_line.strip()\n",
    "                if not tgt_line:\n",
    "                    continue\n",
    "                yield {'src':src_line, 'tgt':tgt_line}\n",
    " # 过滤掉长度 ≤min_len或者≥max_len 的数据            \n",
    "def min_max_filer(data, max_len, min_len=0):\n",
    "    # 1 for special tokens.\n",
    "    data_min_len = min(len(data[0]), len(data[1])) + 1\n",
    "    data_max_len = max(len(data[0]), len(data[1])) + 1\n",
    "    return (data_min_len >= min_len) and (data_max_len <= max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建训练集、验证集的dataloader\n",
    "def create_data_loader(args):\n",
    "    train_dataset = load_dataset(read, src_path=args.training_file.split(',')[0], tgt_path=args.training_file.split(',')[1], lazy=False)\n",
    "    dev_dataset = load_dataset(read, src_path=args.validation_file.split(',')[0], tgt_path=args.validation_file.split(',')[1], lazy=False)\n",
    "\n",
    "    src_vocab = Vocab.load_vocabulary(\n",
    "        args.src_vocab_fpath,\n",
    "        bos_token=args.special_token[0],\n",
    "        eos_token=args.special_token[1],\n",
    "        unk_token=args.special_token[2])\n",
    "    trg_vocab = Vocab.load_vocabulary(\n",
    "        args.trg_vocab_fpath,\n",
    "        bos_token=args.special_token[0],\n",
    "        eos_token=args.special_token[1],\n",
    "        unk_token=args.special_token[2])\n",
    "\n",
    "    padding_vocab = (\n",
    "        lambda x: (x + args.pad_factor - 1) // args.pad_factor * args.pad_factor\n",
    "    )\n",
    "    args.src_vocab_size = padding_vocab(len(src_vocab))\n",
    "    args.trg_vocab_size = padding_vocab(len(trg_vocab))\n",
    "\n",
    "    def convert_samples(sample):\n",
    "        source = sample['src'].split()\n",
    "        target = sample['tgt'].split()\n",
    "\n",
    "        source = src_vocab.to_indices(source)\n",
    "        target = trg_vocab.to_indices(target)\n",
    "\n",
    "        return source, target\n",
    "\n",
    "    # 训练集dataloader和验证集dataloader\n",
    "    data_loaders = []\n",
    "    for i, dataset in enumerate([train_dataset, dev_dataset]):\n",
    "        dataset = dataset.map(convert_samples, lazy=False).filter(\n",
    "            partial(min_max_filer, max_len=args.max_length))\n",
    "\n",
    "        # BatchSampler: https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/io/BatchSampler_cn.html\n",
    "        batch_sampler = BatchSampler(dataset,batch_size=args.batch_size, shuffle=True,drop_last=False)\n",
    "        \n",
    "        # DataLoader: https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/io/DataLoader_cn.html\n",
    "        data_loader = DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_sampler=batch_sampler,\n",
    "            collate_fn=partial(\n",
    "                prepare_train_input,\n",
    "                bos_idx=args.bos_idx,\n",
    "                eos_idx=args.eos_idx,\n",
    "                pad_idx=args.bos_idx),\n",
    "                num_workers=0,\n",
    "                return_list=True)\n",
    "        data_loaders.append(data_loader)\n",
    "\n",
    "    return data_loaders\n",
    "\n",
    "\n",
    "def prepare_train_input(insts, bos_idx, eos_idx, pad_idx):\n",
    "    \"\"\"\n",
    "    Put all padded data needed by training into a list.\n",
    "    \"\"\"\n",
    "    word_pad = Pad(pad_idx)\n",
    "    src_word = word_pad([inst[0] + [eos_idx] for inst in insts])\n",
    "    trg_word = word_pad([[bos_idx] + inst[1] for inst in insts])\n",
    "    lbl_word = np.expand_dims(\n",
    "        word_pad([inst[1] + [eos_idx] for inst in insts]), axis=2)\n",
    "\n",
    "    data_inputs = [src_word, trg_word, lbl_word]\n",
    "\n",
    "    return data_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建测试集的dataloader，原理步骤同上（创建训练集、验证集的dataloader）\n",
    "def create_infer_loader(args):\n",
    "    dataset = load_dataset(read, src_path=args.predict_file, tgt_path=None, is_predict=True, lazy=False)\n",
    "\n",
    "    src_vocab = Vocab.load_vocabulary(\n",
    "        args.src_vocab_fpath,\n",
    "        bos_token=args.special_token[0],\n",
    "        eos_token=args.special_token[1],\n",
    "        unk_token=args.special_token[2])\n",
    "    trg_vocab = Vocab.load_vocabulary(\n",
    "        args.trg_vocab_fpath,\n",
    "        bos_token=args.special_token[0],\n",
    "        eos_token=args.special_token[1],\n",
    "        unk_token=args.special_token[2])\n",
    "\n",
    "    padding_vocab = (\n",
    "        lambda x: (x + args.pad_factor - 1) // args.pad_factor * args.pad_factor\n",
    "    )\n",
    "    args.src_vocab_size = padding_vocab(len(src_vocab))\n",
    "    args.trg_vocab_size = padding_vocab(len(trg_vocab))\n",
    "\n",
    "    def convert_samples(sample):\n",
    "        source = sample['src'].split()\n",
    "        target = sample['tgt'].split()\n",
    "\n",
    "        source = src_vocab.to_indices(source)\n",
    "        target = trg_vocab.to_indices(target)\n",
    "\n",
    "        return source, target\n",
    "\n",
    "    dataset = dataset.map(convert_samples, lazy=False)\n",
    "\n",
    "    # BatchSampler: https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/io/BatchSampler_cn.html\n",
    "    batch_sampler = BatchSampler(dataset,batch_size=args.infer_batch_size,drop_last=False)\n",
    "    \n",
    "    # DataLoader: https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/io/DataLoader_cn.html\n",
    "    data_loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_sampler=batch_sampler,\n",
    "        collate_fn=partial(\n",
    "            prepare_infer_input,\n",
    "            bos_idx=args.bos_idx,\n",
    "            eos_idx=args.eos_idx,\n",
    "            pad_idx=args.bos_idx),\n",
    "            num_workers=0,\n",
    "            return_list=True)\n",
    "    return data_loader, trg_vocab.to_tokens\n",
    "\n",
    "def prepare_infer_input(insts, bos_idx, eos_idx, pad_idx):\n",
    "    \"\"\"\n",
    "    Put all padded data needed by beam search decoder into a list.\n",
    "    \"\"\"\n",
    "    word_pad = Pad(pad_idx)\n",
    "    src_word = word_pad([inst[0] + [eos_idx] for inst in insts])\n",
    "\n",
    "    return [src_word, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4、模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化\n",
    "from visualdl import LogWriter\n",
    "logwriter = LogWriter(logdir='./visualdl/transformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_train(args):\n",
    "    if args.use_gpu:\n",
    "        place = \"gpu\"\n",
    "    else:\n",
    "        place = \"cpu\"\n",
    "    paddle.set_device(place)\n",
    "    # Set seed for CE\n",
    "    random_seed = eval(str(args.random_seed))\n",
    "    if random_seed is not None:\n",
    "        paddle.seed(random_seed)\n",
    "\n",
    "    # Define data loader\n",
    "    (train_loader), (eval_loader) = create_data_loader(args)\n",
    "\n",
    "    # Define model\n",
    "    transformer = TransformerModel( # 用于训练\n",
    "        src_vocab_size=args.src_vocab_size,\n",
    "        trg_vocab_size=args.trg_vocab_size,\n",
    "        max_length=args.max_length + 1,\n",
    "        num_encoder_layers=args.n_layer,\n",
    "        num_decoder_layers=args.n_layer,\n",
    "        n_head=args.n_head,\n",
    "        d_model=args.d_model,\n",
    "        d_inner_hid=args.d_inner_hid,\n",
    "        dropout=args.dropout,\n",
    "        weight_sharing=args.weight_sharing,\n",
    "        bos_id=args.bos_idx,\n",
    "        eos_id=args.eos_idx)\n",
    "\n",
    "    # Define loss\n",
    "    criterion = CrossEntropyCriterion(args.label_smooth_eps, args.bos_idx)\n",
    "\n",
    "    scheduler = paddle.optimizer.lr.NoamDecay(\n",
    "        args.d_model, args.warmup_steps, args.learning_rate, last_epoch=0)\n",
    "\n",
    "    # Define optimizer\n",
    "    optimizer = paddle.optimizer.Adam(\n",
    "        learning_rate=scheduler,\n",
    "        beta1=args.beta1,\n",
    "        beta2=args.beta2,\n",
    "        epsilon=float(args.eps),\n",
    "        parameters=transformer.parameters())\n",
    "\n",
    "    step_idx = 0\n",
    "\n",
    "    # Train loop\n",
    "    for pass_id in range(args.epoch):\n",
    "        batch_id = 0\n",
    "        for input_data in train_loader:\n",
    "\n",
    "            (src_word, trg_word, lbl_word) = input_data\n",
    "\n",
    "            logits = transformer(src_word=src_word, trg_word=trg_word)\n",
    "\n",
    "            sum_cost, avg_cost, token_num = criterion(logits, lbl_word)\n",
    "            \n",
    "            # 计算梯度\n",
    "            avg_cost.backward() \n",
    "            # 更新参数\n",
    "            optimizer.step() \n",
    "            # 梯度清零\n",
    "            optimizer.clear_grad() \n",
    "\n",
    "            if (step_idx + 1) % args.print_step == 0 or step_idx == 0:\n",
    "                total_avg_cost = avg_cost.numpy()\n",
    "                logger.info(\n",
    "                    \"step_idx: %d, epoch: %d, batch: %d, avg loss: %f, \"\n",
    "                    \" ppl: %f \" %\n",
    "                    (step_idx, pass_id, batch_id, total_avg_cost,\n",
    "                        np.exp([min(total_avg_cost, 100)])))\n",
    "                logwriter.add_scalar(\"train_loss\", value=total_avg_cost, step=step_idx+pass_id*(args.batch_size))\n",
    "                logwriter.add_scalar(\"train_perplexity\", value=np.exp([min(total_avg_cost, 100)]), step=step_idx+pass_id*(args.batch_size))\n",
    "\n",
    "            if (step_idx + 1) % args.save_step == 0:\n",
    "                # Validation\n",
    "                transformer.eval()\n",
    "                total_sum_cost = 0\n",
    "                total_token_num = 0\n",
    "                with paddle.no_grad():\n",
    "                    for input_data in eval_loader:\n",
    "                        (src_word, trg_word, lbl_word) = input_data\n",
    "                        logits = transformer(\n",
    "                            src_word=src_word, trg_word=trg_word)\n",
    "                        sum_cost, avg_cost, token_num = criterion(logits,\n",
    "                                                                  lbl_word)\n",
    "                        total_sum_cost += sum_cost.numpy()\n",
    "                        total_token_num += token_num.numpy()\n",
    "                        total_avg_cost = total_sum_cost / total_token_num\n",
    "                    logger.info(\"validation, step_idx: %d, avg loss: %f, \"\n",
    "                                \" ppl: %f\" %\n",
    "                                (step_idx, total_avg_cost,\n",
    "                                 np.exp([min(total_avg_cost, 100)])))\n",
    "                    logwriter.add_scalar(\"valid_loss\", value=total_avg_cost, step=step_idx+pass_id*(args.batch_size))\n",
    "                    logwriter.add_scalar(\"valid_perplexity\", value=np.exp([min(total_avg_cost, 100)]), step=step_idx+pass_id*(args.batch_size))\n",
    "                transformer.train()\n",
    "\n",
    "                if args.save_model:\n",
    "                    model_dir = os.path.join(args.save_model,\n",
    "                                             \"step_\" + str(step_idx))\n",
    "                    if not os.path.exists(model_dir):\n",
    "                        os.makedirs(model_dir)\n",
    "                    paddle.save(transformer.state_dict(),\n",
    "                                os.path.join(model_dir, \"transformer.pdparams\"))\n",
    "                    paddle.save(optimizer.state_dict(),\n",
    "                                os.path.join(model_dir, \"transformer.pdopt\"))\n",
    "            batch_id += 1\n",
    "            step_idx += 1\n",
    "            scheduler.step()\n",
    "\n",
    "\n",
    "    if args.save_model:\n",
    "        model_dir = os.path.join(args.save_model, \"step_final\")\n",
    "        if not os.path.exists(model_dir):\n",
    "            os.makedirs(model_dir)\n",
    "        paddle.save(transformer.state_dict(),\n",
    "                    os.path.join(model_dir, \"transformer.pdparams\"))\n",
    "        paddle.save(optimizer.state_dict(),\n",
    "                    os.path.join(model_dir, \"transformer.pdopt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 50,\n",
      " 'beam_size': 5,\n",
      " 'beta1': 0.9,\n",
      " 'beta2': 0.997,\n",
      " 'bos_idx': 0,\n",
      " 'd_inner_hid': 2048,\n",
      " 'd_model': 512,\n",
      " 'dropout': 0.1,\n",
      " 'eos_idx': 1,\n",
      " 'epoch': 100,\n",
      " 'eps': '1e-9',\n",
      " 'infer_batch_size': 50,\n",
      " 'init_from_params': 'trained_models/CWMT2021_step_345000/',\n",
      " 'label_smooth_eps': 0.1,\n",
      " 'learning_rate': 2.0,\n",
      " 'max_length': 256,\n",
      " 'max_out_len': 256,\n",
      " 'n_best': 1,\n",
      " 'n_head': 8,\n",
      " 'n_layer': 6,\n",
      " 'output_file': './data/train_dev_test/predict.txt',\n",
      " 'pad_factor': 8,\n",
      " 'predict_file': './data/train_dev_test/ccmt2019-news.zh2en.source_cut.txt',\n",
      " 'print_step': 10,\n",
      " 'random_seed': 'None',\n",
      " 'save_model': 'model/transformer',\n",
      " 'save_step': 20,\n",
      " 'special_token': ['<s>', '<e>', '<unk>'],\n",
      " 'src_vocab_fpath': './data/train_dev_test/vocab.ch.src',\n",
      " 'src_vocab_size': 10000,\n",
      " 'training_file': './data/train_dev_test/train.ch.bpe,./data/train_dev_test/train.en.bpe',\n",
      " 'trg_vocab_fpath': './data/train_dev_test/vocab.en.tgt',\n",
      " 'trg_vocab_size': 10000,\n",
      " 'unk_idx': 2,\n",
      " 'use_gpu': True,\n",
      " 'validation_file': './data/train_dev_test/dev.ch.bpe,./data/train_dev_test/dev.en.bpe',\n",
      " 'warmup_steps': 8000,\n",
      " 'weight_sharing': False}\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# 读入参数\n",
    "yaml_file = './helper/transformer.base.yaml'\n",
    "with open(yaml_file, 'rt') as f:\n",
    "    args = AttrDict(yaml.safe_load(f))\n",
    "    pprint(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2024-01-14 03:57:59,997] [    INFO]\u001b[0m - step_idx: 0, epoch: 0, batch: 0, avg loss: 8.517408,  ppl: 5001.076172 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:58:00,653] [    INFO]\u001b[0m - step_idx: 9, epoch: 0, batch: 9, avg loss: 8.508018,  ppl: 4954.336426 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:58:01,382] [    INFO]\u001b[0m - step_idx: 19, epoch: 0, batch: 19, avg loss: 8.429185,  ppl: 4578.766602 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:58:01,464] [    INFO]\u001b[0m - validation, step_idx: 19, avg loss: 8.459351,  ppl: 4718.992188\u001b[0m\n",
      "\u001b[32m[2024-01-14 03:58:04,037] [    INFO]\u001b[0m - step_idx: 29, epoch: 1, batch: 9, avg loss: 8.328434,  ppl: 4139.929199 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:58:04,790] [    INFO]\u001b[0m - step_idx: 39, epoch: 1, batch: 19, avg loss: 8.217314,  ppl: 3704.538086 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:58:04,872] [    INFO]\u001b[0m - validation, step_idx: 39, avg loss: 8.316820,  ppl: 4092.127197\u001b[0m\n",
      "\u001b[32m[2024-01-14 03:58:06,810] [    INFO]\u001b[0m - step_idx: 49, epoch: 2, batch: 9, avg loss: 8.154040,  ppl: 3477.400635 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:58:07,567] [    INFO]\u001b[0m - step_idx: 59, epoch: 2, batch: 19, avg loss: 8.031993,  ppl: 3077.869385 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:58:07,617] [    INFO]\u001b[0m - validation, step_idx: 59, avg loss: 8.203491,  ppl: 3653.683838\u001b[0m\n",
      "\u001b[32m[2024-01-14 03:58:09,778] [    INFO]\u001b[0m - step_idx: 69, epoch: 3, batch: 9, avg loss: 8.033215,  ppl: 3081.631836 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:58:10,491] [    INFO]\u001b[0m - step_idx: 79, epoch: 3, batch: 19, avg loss: 7.902636,  ppl: 2704.401855 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:58:10,573] [    INFO]\u001b[0m - validation, step_idx: 79, avg loss: 8.121174,  ppl: 3364.968262\u001b[0m\n",
      "\u001b[32m[2024-01-14 03:58:12,431] [    INFO]\u001b[0m - step_idx: 89, epoch: 4, batch: 9, avg loss: 7.870298,  ppl: 2618.346680 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:58:13,176] [    INFO]\u001b[0m - step_idx: 99, epoch: 4, batch: 19, avg loss: 7.854357,  ppl: 2576.937012 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:58:13,249] [    INFO]\u001b[0m - validation, step_idx: 99, avg loss: 8.045588,  ppl: 3119.997559\u001b[0m\n",
      "\u001b[32m[2024-01-14 03:58:15,294] [    INFO]\u001b[0m - step_idx: 109, epoch: 5, batch: 9, avg loss: 7.777101,  ppl: 2385.349854 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:58:16,037] [    INFO]\u001b[0m - step_idx: 119, epoch: 5, batch: 19, avg loss: 7.727498,  ppl: 2269.914795 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:58:16,119] [    INFO]\u001b[0m - validation, step_idx: 119, avg loss: 7.972158,  ppl: 2899.107910\u001b[0m\n",
      "\u001b[32m[2024-01-14 03:58:18,224] [    INFO]\u001b[0m - step_idx: 129, epoch: 6, batch: 9, avg loss: 7.743088,  ppl: 2305.580322 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:58:18,988] [    INFO]\u001b[0m - step_idx: 139, epoch: 6, batch: 19, avg loss: 7.719665,  ppl: 2252.205078 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:58:19,070] [    INFO]\u001b[0m - validation, step_idx: 139, avg loss: 7.899318,  ppl: 2695.444092\u001b[0m\n",
      "\u001b[32m[2024-01-14 03:58:21,055] [    INFO]\u001b[0m - step_idx: 149, epoch: 7, batch: 9, avg loss: 7.656989,  ppl: 2115.378662 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:58:21,801] [    INFO]\u001b[0m - step_idx: 159, epoch: 7, batch: 19, avg loss: 7.645250,  ppl: 2090.690918 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:58:21,883] [    INFO]\u001b[0m - validation, step_idx: 159, avg loss: 7.823297,  ppl: 2498.128174\u001b[0m\n",
      "\u001b[32m[2024-01-14 03:58:23,826] [    INFO]\u001b[0m - step_idx: 169, epoch: 8, batch: 9, avg loss: 7.604194,  ppl: 2006.593262 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:58:24,560] [    INFO]\u001b[0m - step_idx: 179, epoch: 8, batch: 19, avg loss: 7.533088,  ppl: 1868.867065 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:58:24,641] [    INFO]\u001b[0m - validation, step_idx: 179, avg loss: 7.749183,  ppl: 2319.675781\u001b[0m\n",
      "\u001b[32m[2024-01-14 03:58:26,796] [    INFO]\u001b[0m - step_idx: 189, epoch: 9, batch: 9, avg loss: 7.514496,  ppl: 1834.442505 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:58:27,423] [    INFO]\u001b[0m - step_idx: 199, epoch: 9, batch: 19, avg loss: 7.484543,  ppl: 1780.310791 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:58:27,485] [    INFO]\u001b[0m - validation, step_idx: 199, avg loss: 7.684193,  ppl: 2173.714111\u001b[0m\n",
      "\u001b[32m[2024-01-14 03:58:29,538] [    INFO]\u001b[0m - step_idx: 209, epoch: 10, batch: 9, avg loss: 7.422825,  ppl: 1673.754883 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:58:30,242] [    INFO]\u001b[0m - step_idx: 219, epoch: 10, batch: 19, avg loss: 7.479941,  ppl: 1772.135986 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:58:30,288] [    INFO]\u001b[0m - validation, step_idx: 219, avg loss: 7.617357,  ppl: 2033.180786\u001b[0m\n",
      "\u001b[32m[2024-01-14 03:58:32,645] [    INFO]\u001b[0m - step_idx: 229, epoch: 11, batch: 9, avg loss: 7.345075,  ppl: 1548.550415 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:58:33,415] [    INFO]\u001b[0m - step_idx: 239, epoch: 11, batch: 19, avg loss: 7.353196,  ppl: 1561.178345 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:58:33,499] [    INFO]\u001b[0m - validation, step_idx: 239, avg loss: 7.570761,  ppl: 1940.616943\u001b[0m\n",
      "\u001b[32m[2024-01-14 03:58:35,705] [    INFO]\u001b[0m - step_idx: 249, epoch: 12, batch: 9, avg loss: 7.326082,  ppl: 1519.417358 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:58:36,430] [    INFO]\u001b[0m - step_idx: 259, epoch: 12, batch: 19, avg loss: 7.244142,  ppl: 1399.880371 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:58:36,513] [    INFO]\u001b[0m - validation, step_idx: 259, avg loss: 7.520397,  ppl: 1845.299072\u001b[0m\n",
      "\u001b[32m[2024-01-14 03:58:38,527] [    INFO]\u001b[0m - step_idx: 269, epoch: 13, batch: 9, avg loss: 7.249162,  ppl: 1406.924927 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:58:39,317] [    INFO]\u001b[0m - step_idx: 279, epoch: 13, batch: 19, avg loss: 7.258685,  ppl: 1420.386963 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:58:39,399] [    INFO]\u001b[0m - validation, step_idx: 279, avg loss: 7.485825,  ppl: 1782.594360\u001b[0m\n",
      "\u001b[32m[2024-01-14 03:58:41,693] [    INFO]\u001b[0m - step_idx: 289, epoch: 14, batch: 9, avg loss: 7.117391,  ppl: 1233.228271 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:58:42,456] [    INFO]\u001b[0m - step_idx: 299, epoch: 14, batch: 19, avg loss: 7.214355,  ppl: 1358.796997 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:58:42,512] [    INFO]\u001b[0m - validation, step_idx: 299, avg loss: 7.464508,  ppl: 1744.996094\u001b[0m\n",
      "\u001b[32m[2024-01-14 03:58:44,975] [    INFO]\u001b[0m - step_idx: 309, epoch: 15, batch: 9, avg loss: 7.097558,  ppl: 1209.010498 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:58:45,743] [    INFO]\u001b[0m - step_idx: 319, epoch: 15, batch: 19, avg loss: 7.131550,  ppl: 1250.814087 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:58:45,827] [    INFO]\u001b[0m - validation, step_idx: 319, avg loss: 7.451604,  ppl: 1722.624634\u001b[0m\n",
      "\u001b[32m[2024-01-14 03:58:48,038] [    INFO]\u001b[0m - step_idx: 329, epoch: 16, batch: 9, avg loss: 7.120458,  ppl: 1237.016357 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:58:48,791] [    INFO]\u001b[0m - step_idx: 339, epoch: 16, batch: 19, avg loss: 7.014039,  ppl: 1112.136963 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:58:48,875] [    INFO]\u001b[0m - validation, step_idx: 339, avg loss: 7.440917,  ppl: 1704.311646\u001b[0m\n",
      "\u001b[32m[2024-01-14 03:58:50,924] [    INFO]\u001b[0m - step_idx: 349, epoch: 17, batch: 9, avg loss: 6.943873,  ppl: 1036.777832 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:58:51,668] [    INFO]\u001b[0m - step_idx: 359, epoch: 17, batch: 19, avg loss: 6.915731,  ppl: 1008.007996 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:58:51,747] [    INFO]\u001b[0m - validation, step_idx: 359, avg loss: 7.389865,  ppl: 1619.488159\u001b[0m\n",
      "\u001b[32m[2024-01-14 03:58:53,925] [    INFO]\u001b[0m - step_idx: 369, epoch: 18, batch: 9, avg loss: 6.877402,  ppl: 970.102600 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:58:54,604] [    INFO]\u001b[0m - step_idx: 379, epoch: 18, batch: 19, avg loss: 6.901499,  ppl: 993.763062 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:58:54,644] [    INFO]\u001b[0m - validation, step_idx: 379, avg loss: 7.377041,  ppl: 1598.851440\u001b[0m\n",
      "\u001b[32m[2024-01-14 03:58:56,545] [    INFO]\u001b[0m - step_idx: 389, epoch: 19, batch: 9, avg loss: 6.795251,  ppl: 893.593811 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:58:57,204] [    INFO]\u001b[0m - step_idx: 399, epoch: 19, batch: 19, avg loss: 6.810113,  ppl: 906.973328 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:58:57,285] [    INFO]\u001b[0m - validation, step_idx: 399, avg loss: 7.364285,  ppl: 1578.587158\u001b[0m\n",
      "\u001b[32m[2024-01-14 03:58:59,179] [    INFO]\u001b[0m - step_idx: 409, epoch: 20, batch: 9, avg loss: 6.672140,  ppl: 790.084717 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:58:59,645] [    INFO]\u001b[0m - step_idx: 419, epoch: 20, batch: 19, avg loss: 6.722270,  ppl: 830.700684 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:58:59,723] [    INFO]\u001b[0m - validation, step_idx: 419, avg loss: 7.365998,  ppl: 1581.293213\u001b[0m\n",
      "\u001b[32m[2024-01-14 03:59:01,640] [    INFO]\u001b[0m - step_idx: 429, epoch: 21, batch: 9, avg loss: 6.648559,  ppl: 771.671631 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:59:02,292] [    INFO]\u001b[0m - step_idx: 439, epoch: 21, batch: 19, avg loss: 6.556562,  ppl: 703.848022 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:59:02,373] [    INFO]\u001b[0m - validation, step_idx: 439, avg loss: 7.334403,  ppl: 1532.112183\u001b[0m\n",
      "\u001b[32m[2024-01-14 03:59:04,441] [    INFO]\u001b[0m - step_idx: 449, epoch: 22, batch: 9, avg loss: 6.542586,  ppl: 694.079102 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:59:05,163] [    INFO]\u001b[0m - step_idx: 459, epoch: 22, batch: 19, avg loss: 6.519553,  ppl: 678.274963 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:59:05,249] [    INFO]\u001b[0m - validation, step_idx: 459, avg loss: 7.362475,  ppl: 1575.732178\u001b[0m\n",
      "\u001b[32m[2024-01-14 03:59:07,328] [    INFO]\u001b[0m - step_idx: 469, epoch: 23, batch: 9, avg loss: 6.374762,  ppl: 586.845520 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:59:08,059] [    INFO]\u001b[0m - step_idx: 479, epoch: 23, batch: 19, avg loss: 6.385612,  ppl: 593.247681 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:59:08,140] [    INFO]\u001b[0m - validation, step_idx: 479, avg loss: 7.320923,  ppl: 1511.598877\u001b[0m\n",
      "\u001b[32m[2024-01-14 03:59:10,064] [    INFO]\u001b[0m - step_idx: 489, epoch: 24, batch: 9, avg loss: 6.295643,  ppl: 542.204346 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:59:10,822] [    INFO]\u001b[0m - step_idx: 499, epoch: 24, batch: 19, avg loss: 6.320615,  ppl: 555.914734 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:59:10,900] [    INFO]\u001b[0m - validation, step_idx: 499, avg loss: 7.301987,  ppl: 1483.243652\u001b[0m\n",
      "\u001b[32m[2024-01-14 03:59:12,970] [    INFO]\u001b[0m - step_idx: 509, epoch: 25, batch: 9, avg loss: 6.247146,  ppl: 516.536377 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:59:13,707] [    INFO]\u001b[0m - step_idx: 519, epoch: 25, batch: 19, avg loss: 6.109503,  ppl: 450.115051 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:59:13,786] [    INFO]\u001b[0m - validation, step_idx: 519, avg loss: 7.292596,  ppl: 1469.380859\u001b[0m\n",
      "\u001b[32m[2024-01-14 03:59:15,765] [    INFO]\u001b[0m - step_idx: 529, epoch: 26, batch: 9, avg loss: 6.123778,  ppl: 456.586609 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:59:16,501] [    INFO]\u001b[0m - step_idx: 539, epoch: 26, batch: 19, avg loss: 6.066034,  ppl: 430.967987 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:59:16,582] [    INFO]\u001b[0m - validation, step_idx: 539, avg loss: 7.287472,  ppl: 1461.869873\u001b[0m\n",
      "\u001b[32m[2024-01-14 03:59:18,581] [    INFO]\u001b[0m - step_idx: 549, epoch: 27, batch: 9, avg loss: 6.076411,  ppl: 435.463593 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:59:19,265] [    INFO]\u001b[0m - step_idx: 559, epoch: 27, batch: 19, avg loss: 5.910427,  ppl: 368.863647 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:59:19,304] [    INFO]\u001b[0m - validation, step_idx: 559, avg loss: 7.281011,  ppl: 1452.455078\u001b[0m\n",
      "\u001b[32m[2024-01-14 03:59:21,268] [    INFO]\u001b[0m - step_idx: 569, epoch: 28, batch: 9, avg loss: 5.964671,  ppl: 389.424927 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:59:21,896] [    INFO]\u001b[0m - step_idx: 579, epoch: 28, batch: 19, avg loss: 5.930481,  ppl: 376.335663 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:59:21,977] [    INFO]\u001b[0m - validation, step_idx: 579, avg loss: 7.293575,  ppl: 1470.819092\u001b[0m\n",
      "\u001b[32m[2024-01-14 03:59:23,893] [    INFO]\u001b[0m - step_idx: 589, epoch: 29, batch: 9, avg loss: 5.812150,  ppl: 334.337341 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:59:24,599] [    INFO]\u001b[0m - step_idx: 599, epoch: 29, batch: 19, avg loss: 5.733328,  ppl: 308.996002 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:59:24,678] [    INFO]\u001b[0m - validation, step_idx: 599, avg loss: 7.265867,  ppl: 1430.625122\u001b[0m\n",
      "\u001b[32m[2024-01-14 03:59:26,658] [    INFO]\u001b[0m - step_idx: 609, epoch: 30, batch: 9, avg loss: 5.579332,  ppl: 264.894714 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:59:27,390] [    INFO]\u001b[0m - step_idx: 619, epoch: 30, batch: 19, avg loss: 5.731571,  ppl: 308.453400 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:59:27,472] [    INFO]\u001b[0m - validation, step_idx: 619, avg loss: 7.266287,  ppl: 1431.226318\u001b[0m\n",
      "\u001b[32m[2024-01-14 03:59:29,421] [    INFO]\u001b[0m - step_idx: 629, epoch: 31, batch: 9, avg loss: 5.538217,  ppl: 254.224197 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:59:30,157] [    INFO]\u001b[0m - step_idx: 639, epoch: 31, batch: 19, avg loss: 5.506809,  ppl: 246.363800 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:59:30,241] [    INFO]\u001b[0m - validation, step_idx: 639, avg loss: 7.271791,  ppl: 1439.125610\u001b[0m\n",
      "\u001b[32m[2024-01-14 03:59:32,186] [    INFO]\u001b[0m - step_idx: 649, epoch: 32, batch: 9, avg loss: 5.421684,  ppl: 226.259872 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:59:32,932] [    INFO]\u001b[0m - step_idx: 659, epoch: 32, batch: 19, avg loss: 5.546778,  ppl: 256.410004 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:59:33,014] [    INFO]\u001b[0m - validation, step_idx: 659, avg loss: 7.284212,  ppl: 1457.112671\u001b[0m\n",
      "\u001b[32m[2024-01-14 03:59:34,944] [    INFO]\u001b[0m - step_idx: 669, epoch: 33, batch: 9, avg loss: 5.273947,  ppl: 195.184875 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:59:35,709] [    INFO]\u001b[0m - step_idx: 679, epoch: 33, batch: 19, avg loss: 5.352158,  ppl: 211.063293 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:59:35,790] [    INFO]\u001b[0m - validation, step_idx: 679, avg loss: 7.222108,  ppl: 1369.373169\u001b[0m\n",
      "\u001b[32m[2024-01-14 03:59:37,841] [    INFO]\u001b[0m - step_idx: 689, epoch: 34, batch: 9, avg loss: 5.188805,  ppl: 179.254227 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:59:38,579] [    INFO]\u001b[0m - step_idx: 699, epoch: 34, batch: 19, avg loss: 5.253473,  ppl: 191.229294 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:59:38,655] [    INFO]\u001b[0m - validation, step_idx: 699, avg loss: 7.248252,  ppl: 1405.645630\u001b[0m\n",
      "\u001b[32m[2024-01-14 03:59:40,704] [    INFO]\u001b[0m - step_idx: 709, epoch: 35, batch: 9, avg loss: 5.047800,  ppl: 155.679596 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:59:41,432] [    INFO]\u001b[0m - step_idx: 719, epoch: 35, batch: 19, avg loss: 5.035538,  ppl: 153.782333 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:59:41,472] [    INFO]\u001b[0m - validation, step_idx: 719, avg loss: 7.237851,  ppl: 1391.101440\u001b[0m\n",
      "\u001b[32m[2024-01-14 03:59:43,392] [    INFO]\u001b[0m - step_idx: 729, epoch: 36, batch: 9, avg loss: 4.930928,  ppl: 138.508011 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:59:44,003] [    INFO]\u001b[0m - step_idx: 739, epoch: 36, batch: 19, avg loss: 5.033662,  ppl: 153.494049 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:59:44,047] [    INFO]\u001b[0m - validation, step_idx: 739, avg loss: 7.248642,  ppl: 1406.194702\u001b[0m\n",
      "\u001b[32m[2024-01-14 03:59:45,950] [    INFO]\u001b[0m - step_idx: 749, epoch: 37, batch: 9, avg loss: 4.893335,  ppl: 133.397690 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:59:46,556] [    INFO]\u001b[0m - step_idx: 759, epoch: 37, batch: 19, avg loss: 4.956875,  ppl: 142.148941 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:59:46,617] [    INFO]\u001b[0m - validation, step_idx: 759, avg loss: 7.231682,  ppl: 1382.546509\u001b[0m\n",
      "\u001b[32m[2024-01-14 03:59:48,530] [    INFO]\u001b[0m - step_idx: 769, epoch: 38, batch: 9, avg loss: 4.729345,  ppl: 113.221413 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:59:49,161] [    INFO]\u001b[0m - step_idx: 779, epoch: 38, batch: 19, avg loss: 4.642934,  ppl: 103.848633 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:59:49,240] [    INFO]\u001b[0m - validation, step_idx: 779, avg loss: 7.253478,  ppl: 1413.010864\u001b[0m\n",
      "\u001b[32m[2024-01-14 03:59:51,162] [    INFO]\u001b[0m - step_idx: 789, epoch: 39, batch: 9, avg loss: 4.657525,  ppl: 105.374954 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:59:51,868] [    INFO]\u001b[0m - step_idx: 799, epoch: 39, batch: 19, avg loss: 4.653622,  ppl: 104.964447 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:59:51,949] [    INFO]\u001b[0m - validation, step_idx: 799, avg loss: 7.256330,  ppl: 1417.047119\u001b[0m\n",
      "\u001b[32m[2024-01-14 03:59:53,788] [    INFO]\u001b[0m - step_idx: 809, epoch: 40, batch: 9, avg loss: 4.429503,  ppl: 83.889748 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:59:54,522] [    INFO]\u001b[0m - step_idx: 819, epoch: 40, batch: 19, avg loss: 4.549778,  ppl: 94.611397 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:59:54,603] [    INFO]\u001b[0m - validation, step_idx: 819, avg loss: 7.213795,  ppl: 1358.035889\u001b[0m\n",
      "\u001b[32m[2024-01-14 03:59:56,453] [    INFO]\u001b[0m - step_idx: 829, epoch: 41, batch: 9, avg loss: 4.382593,  ppl: 80.045296 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:59:57,184] [    INFO]\u001b[0m - step_idx: 839, epoch: 41, batch: 19, avg loss: 4.370034,  ppl: 79.046341 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:59:57,265] [    INFO]\u001b[0m - validation, step_idx: 839, avg loss: 7.232121,  ppl: 1383.153809\u001b[0m\n",
      "\u001b[32m[2024-01-14 03:59:59,008] [    INFO]\u001b[0m - step_idx: 849, epoch: 42, batch: 9, avg loss: 4.362738,  ppl: 78.471664 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:59:59,737] [    INFO]\u001b[0m - step_idx: 859, epoch: 42, batch: 19, avg loss: 4.241363,  ppl: 69.502495 \u001b[0m\n",
      "\u001b[32m[2024-01-14 03:59:59,815] [    INFO]\u001b[0m - validation, step_idx: 859, avg loss: 7.240557,  ppl: 1394.870239\u001b[0m\n",
      "\u001b[32m[2024-01-14 04:00:01,771] [    INFO]\u001b[0m - step_idx: 869, epoch: 43, batch: 9, avg loss: 4.188981,  ppl: 65.955551 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:00:02,522] [    INFO]\u001b[0m - step_idx: 879, epoch: 43, batch: 19, avg loss: 4.128781,  ppl: 62.102169 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:00:02,604] [    INFO]\u001b[0m - validation, step_idx: 879, avg loss: 7.242719,  ppl: 1397.889282\u001b[0m\n",
      "\u001b[32m[2024-01-14 04:00:04,612] [    INFO]\u001b[0m - step_idx: 889, epoch: 44, batch: 9, avg loss: 3.999966,  ppl: 54.596272 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:00:05,364] [    INFO]\u001b[0m - step_idx: 899, epoch: 44, batch: 19, avg loss: 4.080495,  ppl: 59.174747 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:00:05,445] [    INFO]\u001b[0m - validation, step_idx: 899, avg loss: 7.254125,  ppl: 1413.925415\u001b[0m\n",
      "\u001b[32m[2024-01-14 04:00:07,307] [    INFO]\u001b[0m - step_idx: 909, epoch: 45, batch: 9, avg loss: 3.953101,  ppl: 52.096649 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:00:08,068] [    INFO]\u001b[0m - step_idx: 919, epoch: 45, batch: 19, avg loss: 3.938597,  ppl: 51.346493 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:00:08,150] [    INFO]\u001b[0m - validation, step_idx: 919, avg loss: 7.229794,  ppl: 1379.937744\u001b[0m\n",
      "\u001b[32m[2024-01-14 04:00:10,018] [    INFO]\u001b[0m - step_idx: 929, epoch: 46, batch: 9, avg loss: 3.754668,  ppl: 42.720016 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:00:10,768] [    INFO]\u001b[0m - step_idx: 939, epoch: 46, batch: 19, avg loss: 3.693001,  ppl: 40.165215 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:00:10,853] [    INFO]\u001b[0m - validation, step_idx: 939, avg loss: 7.240611,  ppl: 1394.946167\u001b[0m\n",
      "\u001b[32m[2024-01-14 04:00:12,863] [    INFO]\u001b[0m - step_idx: 949, epoch: 47, batch: 9, avg loss: 3.665975,  ppl: 39.094223 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:00:13,601] [    INFO]\u001b[0m - step_idx: 959, epoch: 47, batch: 19, avg loss: 3.629821,  ppl: 37.706051 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:00:13,682] [    INFO]\u001b[0m - validation, step_idx: 959, avg loss: 7.254725,  ppl: 1414.773193\u001b[0m\n",
      "\u001b[32m[2024-01-14 04:00:15,597] [    INFO]\u001b[0m - step_idx: 969, epoch: 48, batch: 9, avg loss: 3.536773,  ppl: 34.355888 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:00:16,344] [    INFO]\u001b[0m - step_idx: 979, epoch: 48, batch: 19, avg loss: 3.460862,  ppl: 31.844429 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:00:16,413] [    INFO]\u001b[0m - validation, step_idx: 979, avg loss: 7.291181,  ppl: 1467.301880\u001b[0m\n",
      "\u001b[32m[2024-01-14 04:00:18,401] [    INFO]\u001b[0m - step_idx: 989, epoch: 49, batch: 9, avg loss: 3.396499,  ppl: 29.859385 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:00:19,060] [    INFO]\u001b[0m - step_idx: 999, epoch: 49, batch: 19, avg loss: 3.422368,  ppl: 30.641874 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:00:19,139] [    INFO]\u001b[0m - validation, step_idx: 999, avg loss: 7.294174,  ppl: 1471.700195\u001b[0m\n",
      "\u001b[32m[2024-01-14 04:00:21,074] [    INFO]\u001b[0m - step_idx: 1009, epoch: 50, batch: 9, avg loss: 3.267392,  ppl: 26.242813 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:00:21,800] [    INFO]\u001b[0m - step_idx: 1019, epoch: 50, batch: 19, avg loss: 3.226621,  ppl: 25.194391 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:00:21,877] [    INFO]\u001b[0m - validation, step_idx: 1019, avg loss: 7.277021,  ppl: 1446.671875\u001b[0m\n",
      "\u001b[32m[2024-01-14 04:00:23,793] [    INFO]\u001b[0m - step_idx: 1029, epoch: 51, batch: 9, avg loss: 3.153043,  ppl: 23.407177 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:00:24,519] [    INFO]\u001b[0m - step_idx: 1039, epoch: 51, batch: 19, avg loss: 3.175944,  ppl: 23.949408 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:00:24,601] [    INFO]\u001b[0m - validation, step_idx: 1039, avg loss: 7.283271,  ppl: 1455.742310\u001b[0m\n",
      "\u001b[32m[2024-01-14 04:00:26,462] [    INFO]\u001b[0m - step_idx: 1049, epoch: 52, batch: 9, avg loss: 3.070186,  ppl: 21.545902 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:00:27,199] [    INFO]\u001b[0m - step_idx: 1059, epoch: 52, batch: 19, avg loss: 3.017166,  ppl: 20.433304 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:00:27,280] [    INFO]\u001b[0m - validation, step_idx: 1059, avg loss: 7.321643,  ppl: 1512.687866\u001b[0m\n",
      "\u001b[32m[2024-01-14 04:00:29,256] [    INFO]\u001b[0m - step_idx: 1069, epoch: 53, batch: 9, avg loss: 2.904700,  ppl: 18.259766 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:00:30,000] [    INFO]\u001b[0m - step_idx: 1079, epoch: 53, batch: 19, avg loss: 2.869567,  ppl: 17.629377 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:00:30,081] [    INFO]\u001b[0m - validation, step_idx: 1079, avg loss: 7.308514,  ppl: 1492.957153\u001b[0m\n",
      "\u001b[32m[2024-01-14 04:00:31,961] [    INFO]\u001b[0m - step_idx: 1089, epoch: 54, batch: 9, avg loss: 2.765876,  ppl: 15.892953 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:00:32,723] [    INFO]\u001b[0m - step_idx: 1099, epoch: 54, batch: 19, avg loss: 2.833554,  ppl: 17.005796 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:00:32,803] [    INFO]\u001b[0m - validation, step_idx: 1099, avg loss: 7.352080,  ppl: 1559.437256\u001b[0m\n",
      "\u001b[32m[2024-01-14 04:00:35,117] [    INFO]\u001b[0m - step_idx: 1109, epoch: 55, batch: 9, avg loss: 2.618659,  ppl: 13.717310 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:00:35,883] [    INFO]\u001b[0m - step_idx: 1119, epoch: 55, batch: 19, avg loss: 2.683877,  ppl: 14.641752 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:00:35,964] [    INFO]\u001b[0m - validation, step_idx: 1119, avg loss: 7.377758,  ppl: 1599.997803\u001b[0m\n",
      "\u001b[32m[2024-01-14 04:00:38,345] [    INFO]\u001b[0m - step_idx: 1129, epoch: 56, batch: 9, avg loss: 2.523086,  ppl: 12.467013 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:00:39,083] [    INFO]\u001b[0m - step_idx: 1139, epoch: 56, batch: 19, avg loss: 2.533007,  ppl: 12.591311 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:00:39,166] [    INFO]\u001b[0m - validation, step_idx: 1139, avg loss: 7.387702,  ppl: 1615.987427\u001b[0m\n",
      "\u001b[32m[2024-01-14 04:00:41,558] [    INFO]\u001b[0m - step_idx: 1149, epoch: 57, batch: 9, avg loss: 2.452306,  ppl: 11.615103 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:00:42,328] [    INFO]\u001b[0m - step_idx: 1159, epoch: 57, batch: 19, avg loss: 2.416494,  ppl: 11.206496 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:00:42,409] [    INFO]\u001b[0m - validation, step_idx: 1159, avg loss: 7.373364,  ppl: 1592.983521\u001b[0m\n",
      "\u001b[32m[2024-01-14 04:00:44,746] [    INFO]\u001b[0m - step_idx: 1169, epoch: 58, batch: 9, avg loss: 2.354668,  ppl: 10.534628 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:00:45,526] [    INFO]\u001b[0m - step_idx: 1179, epoch: 58, batch: 19, avg loss: 2.334572,  ppl: 10.325044 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:00:45,609] [    INFO]\u001b[0m - validation, step_idx: 1179, avg loss: 7.406605,  ppl: 1646.826294\u001b[0m\n",
      "\u001b[32m[2024-01-14 04:00:47,792] [    INFO]\u001b[0m - step_idx: 1189, epoch: 59, batch: 9, avg loss: 2.219795,  ppl: 9.205447 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:00:48,431] [    INFO]\u001b[0m - step_idx: 1199, epoch: 59, batch: 19, avg loss: 2.225372,  ppl: 9.256925 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:00:48,491] [    INFO]\u001b[0m - validation, step_idx: 1199, avg loss: 7.399374,  ppl: 1634.960571\u001b[0m\n",
      "\u001b[32m[2024-01-14 04:00:50,513] [    INFO]\u001b[0m - step_idx: 1209, epoch: 60, batch: 9, avg loss: 2.142811,  ppl: 8.523359 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:00:51,150] [    INFO]\u001b[0m - step_idx: 1219, epoch: 60, batch: 19, avg loss: 2.109889,  ppl: 8.247326 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:00:51,231] [    INFO]\u001b[0m - validation, step_idx: 1219, avg loss: 7.422560,  ppl: 1673.312012\u001b[0m\n",
      "\u001b[32m[2024-01-14 04:00:53,038] [    INFO]\u001b[0m - step_idx: 1229, epoch: 61, batch: 9, avg loss: 2.039458,  ppl: 7.686440 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:00:53,805] [    INFO]\u001b[0m - step_idx: 1239, epoch: 61, batch: 19, avg loss: 2.030827,  ppl: 7.620385 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:00:53,889] [    INFO]\u001b[0m - validation, step_idx: 1239, avg loss: 7.438510,  ppl: 1700.214844\u001b[0m\n",
      "\u001b[32m[2024-01-14 04:00:55,777] [    INFO]\u001b[0m - step_idx: 1249, epoch: 62, batch: 9, avg loss: 1.942882,  ppl: 6.978838 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:00:56,510] [    INFO]\u001b[0m - step_idx: 1259, epoch: 62, batch: 19, avg loss: 1.933714,  ppl: 6.915145 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:00:56,592] [    INFO]\u001b[0m - validation, step_idx: 1259, avg loss: 7.453526,  ppl: 1725.938965\u001b[0m\n",
      "\u001b[32m[2024-01-14 04:00:58,629] [    INFO]\u001b[0m - step_idx: 1269, epoch: 63, batch: 9, avg loss: 1.866985,  ppl: 6.468765 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:00:59,366] [    INFO]\u001b[0m - step_idx: 1279, epoch: 63, batch: 19, avg loss: 1.858931,  ppl: 6.416873 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:00:59,447] [    INFO]\u001b[0m - validation, step_idx: 1279, avg loss: 7.488034,  ppl: 1786.536743\u001b[0m\n",
      "\u001b[32m[2024-01-14 04:01:01,429] [    INFO]\u001b[0m - step_idx: 1289, epoch: 64, batch: 9, avg loss: 1.820981,  ppl: 6.177917 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:01:02,170] [    INFO]\u001b[0m - step_idx: 1299, epoch: 64, batch: 19, avg loss: 1.790244,  ppl: 5.990911 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:01:02,251] [    INFO]\u001b[0m - validation, step_idx: 1299, avg loss: 7.490897,  ppl: 1791.657959\u001b[0m\n",
      "\u001b[32m[2024-01-14 04:01:04,152] [    INFO]\u001b[0m - step_idx: 1309, epoch: 65, batch: 9, avg loss: 1.737807,  ppl: 5.684863 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:01:04,921] [    INFO]\u001b[0m - step_idx: 1319, epoch: 65, batch: 19, avg loss: 1.720213,  ppl: 5.585716 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:01:05,005] [    INFO]\u001b[0m - validation, step_idx: 1319, avg loss: 7.518838,  ppl: 1842.425049\u001b[0m\n",
      "\u001b[32m[2024-01-14 04:01:07,011] [    INFO]\u001b[0m - step_idx: 1329, epoch: 66, batch: 9, avg loss: 1.665985,  ppl: 5.290883 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:01:07,770] [    INFO]\u001b[0m - step_idx: 1339, epoch: 66, batch: 19, avg loss: 1.671720,  ppl: 5.321313 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:01:07,850] [    INFO]\u001b[0m - validation, step_idx: 1339, avg loss: 7.536735,  ppl: 1875.694946\u001b[0m\n",
      "\u001b[32m[2024-01-14 04:01:09,789] [    INFO]\u001b[0m - step_idx: 1349, epoch: 67, batch: 9, avg loss: 1.615516,  ppl: 5.030485 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:01:10,552] [    INFO]\u001b[0m - step_idx: 1359, epoch: 67, batch: 19, avg loss: 1.612764,  ppl: 5.016659 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:01:10,634] [    INFO]\u001b[0m - validation, step_idx: 1359, avg loss: 7.543795,  ppl: 1888.984375\u001b[0m\n",
      "\u001b[32m[2024-01-14 04:01:12,722] [    INFO]\u001b[0m - step_idx: 1369, epoch: 68, batch: 9, avg loss: 1.566432,  ppl: 4.789530 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:01:13,414] [    INFO]\u001b[0m - step_idx: 1379, epoch: 68, batch: 19, avg loss: 1.570810,  ppl: 4.810545 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:01:13,498] [    INFO]\u001b[0m - validation, step_idx: 1379, avg loss: 7.588899,  ppl: 1976.136963\u001b[0m\n",
      "\u001b[32m[2024-01-14 04:01:15,477] [    INFO]\u001b[0m - step_idx: 1389, epoch: 69, batch: 9, avg loss: 1.516731,  ppl: 4.557302 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:01:16,199] [    INFO]\u001b[0m - step_idx: 1399, epoch: 69, batch: 19, avg loss: 1.530227,  ppl: 4.619226 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:01:16,281] [    INFO]\u001b[0m - validation, step_idx: 1399, avg loss: 7.627607,  ppl: 2054.128418\u001b[0m\n",
      "\u001b[32m[2024-01-14 04:01:18,268] [    INFO]\u001b[0m - step_idx: 1409, epoch: 70, batch: 9, avg loss: 1.492060,  ppl: 4.446245 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:01:19,034] [    INFO]\u001b[0m - step_idx: 1419, epoch: 70, batch: 19, avg loss: 1.499126,  ppl: 4.477772 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:01:19,114] [    INFO]\u001b[0m - validation, step_idx: 1419, avg loss: 7.607931,  ppl: 2014.105835\u001b[0m\n",
      "\u001b[32m[2024-01-14 04:01:21,278] [    INFO]\u001b[0m - step_idx: 1429, epoch: 71, batch: 9, avg loss: 1.459419,  ppl: 4.303456 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:01:22,032] [    INFO]\u001b[0m - step_idx: 1439, epoch: 71, batch: 19, avg loss: 1.467891,  ppl: 4.340071 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:01:22,114] [    INFO]\u001b[0m - validation, step_idx: 1439, avg loss: 7.610868,  ppl: 2020.031738\u001b[0m\n",
      "\u001b[32m[2024-01-14 04:01:24,125] [    INFO]\u001b[0m - step_idx: 1449, epoch: 72, batch: 9, avg loss: 1.439299,  ppl: 4.217737 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:01:24,905] [    INFO]\u001b[0m - step_idx: 1459, epoch: 72, batch: 19, avg loss: 1.435437,  ppl: 4.201480 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:01:24,989] [    INFO]\u001b[0m - validation, step_idx: 1459, avg loss: 7.628806,  ppl: 2056.592041\u001b[0m\n",
      "\u001b[32m[2024-01-14 04:01:27,184] [    INFO]\u001b[0m - step_idx: 1469, epoch: 73, batch: 9, avg loss: 1.425247,  ppl: 4.158884 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:01:27,939] [    INFO]\u001b[0m - step_idx: 1479, epoch: 73, batch: 19, avg loss: 1.409486,  ppl: 4.093849 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:01:28,003] [    INFO]\u001b[0m - validation, step_idx: 1479, avg loss: 7.660973,  ppl: 2123.822021\u001b[0m\n",
      "\u001b[32m[2024-01-14 04:01:29,989] [    INFO]\u001b[0m - step_idx: 1489, epoch: 74, batch: 9, avg loss: 1.394469,  ppl: 4.032834 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:01:30,666] [    INFO]\u001b[0m - step_idx: 1499, epoch: 74, batch: 19, avg loss: 1.405913,  ppl: 4.079249 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:01:30,750] [    INFO]\u001b[0m - validation, step_idx: 1499, avg loss: 7.673125,  ppl: 2149.788574\u001b[0m\n",
      "\u001b[32m[2024-01-14 04:01:32,646] [    INFO]\u001b[0m - step_idx: 1509, epoch: 75, batch: 9, avg loss: 1.400077,  ppl: 4.055511 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:01:33,388] [    INFO]\u001b[0m - step_idx: 1519, epoch: 75, batch: 19, avg loss: 1.393002,  ppl: 4.026919 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:01:33,471] [    INFO]\u001b[0m - validation, step_idx: 1519, avg loss: 7.687073,  ppl: 2179.983887\u001b[0m\n",
      "\u001b[32m[2024-01-14 04:01:35,638] [    INFO]\u001b[0m - step_idx: 1529, epoch: 76, batch: 9, avg loss: 1.368464,  ppl: 3.929309 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:01:36,372] [    INFO]\u001b[0m - step_idx: 1539, epoch: 76, batch: 19, avg loss: 1.372804,  ppl: 3.946402 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:01:36,455] [    INFO]\u001b[0m - validation, step_idx: 1539, avg loss: 7.669931,  ppl: 2142.934326\u001b[0m\n",
      "\u001b[32m[2024-01-14 04:01:38,464] [    INFO]\u001b[0m - step_idx: 1549, epoch: 77, batch: 9, avg loss: 1.358857,  ppl: 3.891741 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:01:39,203] [    INFO]\u001b[0m - step_idx: 1559, epoch: 77, batch: 19, avg loss: 1.365120,  ppl: 3.916194 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:01:39,285] [    INFO]\u001b[0m - validation, step_idx: 1559, avg loss: 7.713609,  ppl: 2238.606201\u001b[0m\n",
      "\u001b[32m[2024-01-14 04:01:41,312] [    INFO]\u001b[0m - step_idx: 1569, epoch: 78, batch: 9, avg loss: 1.344205,  ppl: 3.835138 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:01:42,078] [    INFO]\u001b[0m - step_idx: 1579, epoch: 78, batch: 19, avg loss: 1.346748,  ppl: 3.844902 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:01:42,157] [    INFO]\u001b[0m - validation, step_idx: 1579, avg loss: 7.703686,  ppl: 2216.503418\u001b[0m\n",
      "\u001b[32m[2024-01-14 04:01:44,229] [    INFO]\u001b[0m - step_idx: 1589, epoch: 79, batch: 9, avg loss: 1.336973,  ppl: 3.807500 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:01:44,975] [    INFO]\u001b[0m - step_idx: 1599, epoch: 79, batch: 19, avg loss: 1.341957,  ppl: 3.826525 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:01:45,055] [    INFO]\u001b[0m - validation, step_idx: 1599, avg loss: 7.752101,  ppl: 2326.456055\u001b[0m\n",
      "\u001b[32m[2024-01-14 04:01:47,029] [    INFO]\u001b[0m - step_idx: 1609, epoch: 80, batch: 9, avg loss: 1.337678,  ppl: 3.810188 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:01:47,726] [    INFO]\u001b[0m - step_idx: 1619, epoch: 80, batch: 19, avg loss: 1.331887,  ppl: 3.788186 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:01:47,774] [    INFO]\u001b[0m - validation, step_idx: 1619, avg loss: 7.784775,  ppl: 2403.725830\u001b[0m\n",
      "\u001b[32m[2024-01-14 04:01:49,905] [    INFO]\u001b[0m - step_idx: 1629, epoch: 81, batch: 9, avg loss: 1.334597,  ppl: 3.798463 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:01:50,556] [    INFO]\u001b[0m - step_idx: 1639, epoch: 81, batch: 19, avg loss: 1.332704,  ppl: 3.791282 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:01:50,637] [    INFO]\u001b[0m - validation, step_idx: 1639, avg loss: 7.746407,  ppl: 2313.246094\u001b[0m\n",
      "\u001b[32m[2024-01-14 04:01:52,524] [    INFO]\u001b[0m - step_idx: 1649, epoch: 82, batch: 9, avg loss: 1.318645,  ppl: 3.738351 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:01:53,244] [    INFO]\u001b[0m - step_idx: 1659, epoch: 82, batch: 19, avg loss: 1.330169,  ppl: 3.781681 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:01:53,326] [    INFO]\u001b[0m - validation, step_idx: 1659, avg loss: 7.771244,  ppl: 2371.418457\u001b[0m\n",
      "\u001b[32m[2024-01-14 04:01:55,266] [    INFO]\u001b[0m - step_idx: 1669, epoch: 83, batch: 9, avg loss: 1.318473,  ppl: 3.737711 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:01:55,991] [    INFO]\u001b[0m - step_idx: 1679, epoch: 83, batch: 19, avg loss: 1.319818,  ppl: 3.742741 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:01:56,073] [    INFO]\u001b[0m - validation, step_idx: 1679, avg loss: 7.763676,  ppl: 2353.540771\u001b[0m\n",
      "\u001b[32m[2024-01-14 04:01:57,985] [    INFO]\u001b[0m - step_idx: 1689, epoch: 84, batch: 9, avg loss: 1.315832,  ppl: 3.727851 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:01:58,764] [    INFO]\u001b[0m - step_idx: 1699, epoch: 84, batch: 19, avg loss: 1.313426,  ppl: 3.718893 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:01:58,849] [    INFO]\u001b[0m - validation, step_idx: 1699, avg loss: 7.769749,  ppl: 2367.877441\u001b[0m\n",
      "\u001b[32m[2024-01-14 04:02:00,855] [    INFO]\u001b[0m - step_idx: 1709, epoch: 85, batch: 9, avg loss: 1.311462,  ppl: 3.711596 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:02:01,591] [    INFO]\u001b[0m - step_idx: 1719, epoch: 85, batch: 19, avg loss: 1.307057,  ppl: 3.695283 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:02:01,672] [    INFO]\u001b[0m - validation, step_idx: 1719, avg loss: 7.766562,  ppl: 2360.343750\u001b[0m\n",
      "\u001b[32m[2024-01-14 04:02:03,637] [    INFO]\u001b[0m - step_idx: 1729, epoch: 86, batch: 9, avg loss: 1.305397,  ppl: 3.689155 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:02:04,368] [    INFO]\u001b[0m - step_idx: 1739, epoch: 86, batch: 19, avg loss: 1.301911,  ppl: 3.676316 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:02:04,452] [    INFO]\u001b[0m - validation, step_idx: 1739, avg loss: 7.772511,  ppl: 2374.426025\u001b[0m\n",
      "\u001b[32m[2024-01-14 04:02:06,638] [    INFO]\u001b[0m - step_idx: 1749, epoch: 87, batch: 9, avg loss: 1.304059,  ppl: 3.684222 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:02:07,397] [    INFO]\u001b[0m - step_idx: 1759, epoch: 87, batch: 19, avg loss: 1.305899,  ppl: 3.691005 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:02:07,481] [    INFO]\u001b[0m - validation, step_idx: 1759, avg loss: 7.783413,  ppl: 2400.454590\u001b[0m\n",
      "\u001b[32m[2024-01-14 04:02:09,596] [    INFO]\u001b[0m - step_idx: 1769, epoch: 88, batch: 9, avg loss: 1.297073,  ppl: 3.658572 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:02:10,309] [    INFO]\u001b[0m - step_idx: 1779, epoch: 88, batch: 19, avg loss: 1.299453,  ppl: 3.667290 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:02:10,391] [    INFO]\u001b[0m - validation, step_idx: 1779, avg loss: 7.773428,  ppl: 2376.604492\u001b[0m\n",
      "\u001b[32m[2024-01-14 04:02:12,494] [    INFO]\u001b[0m - step_idx: 1789, epoch: 89, batch: 9, avg loss: 1.301308,  ppl: 3.674100 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:02:13,255] [    INFO]\u001b[0m - step_idx: 1799, epoch: 89, batch: 19, avg loss: 1.306597,  ppl: 3.693584 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:02:13,338] [    INFO]\u001b[0m - validation, step_idx: 1799, avg loss: 7.774714,  ppl: 2379.662598\u001b[0m\n",
      "\u001b[32m[2024-01-14 04:02:15,493] [    INFO]\u001b[0m - step_idx: 1809, epoch: 90, batch: 9, avg loss: 1.296494,  ppl: 3.656455 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:02:16,286] [    INFO]\u001b[0m - step_idx: 1819, epoch: 90, batch: 19, avg loss: 1.308620,  ppl: 3.701063 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:02:16,370] [    INFO]\u001b[0m - validation, step_idx: 1819, avg loss: 7.800889,  ppl: 2442.772461\u001b[0m\n",
      "\u001b[32m[2024-01-14 04:02:18,767] [    INFO]\u001b[0m - step_idx: 1829, epoch: 91, batch: 9, avg loss: 1.294420,  ppl: 3.648878 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:02:19,501] [    INFO]\u001b[0m - step_idx: 1839, epoch: 91, batch: 19, avg loss: 1.301804,  ppl: 3.675924 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:02:19,584] [    INFO]\u001b[0m - validation, step_idx: 1839, avg loss: 7.781072,  ppl: 2394.841064\u001b[0m\n",
      "\u001b[32m[2024-01-14 04:02:21,889] [    INFO]\u001b[0m - step_idx: 1849, epoch: 92, batch: 9, avg loss: 1.301992,  ppl: 3.676611 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:02:22,558] [    INFO]\u001b[0m - step_idx: 1859, epoch: 92, batch: 19, avg loss: 1.292213,  ppl: 3.640835 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:02:22,640] [    INFO]\u001b[0m - validation, step_idx: 1859, avg loss: 7.781343,  ppl: 2395.489746\u001b[0m\n",
      "\u001b[32m[2024-01-14 04:02:24,645] [    INFO]\u001b[0m - step_idx: 1869, epoch: 93, batch: 9, avg loss: 1.291019,  ppl: 3.636489 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:02:25,415] [    INFO]\u001b[0m - step_idx: 1879, epoch: 93, batch: 19, avg loss: 1.297293,  ppl: 3.659377 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:02:25,496] [    INFO]\u001b[0m - validation, step_idx: 1879, avg loss: 7.762977,  ppl: 2351.895020\u001b[0m\n",
      "\u001b[32m[2024-01-14 04:02:27,856] [    INFO]\u001b[0m - step_idx: 1889, epoch: 94, batch: 9, avg loss: 1.294627,  ppl: 3.649636 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:02:28,597] [    INFO]\u001b[0m - step_idx: 1899, epoch: 94, batch: 19, avg loss: 1.292719,  ppl: 3.642677 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:02:28,679] [    INFO]\u001b[0m - validation, step_idx: 1899, avg loss: 7.816986,  ppl: 2482.412354\u001b[0m\n",
      "\u001b[32m[2024-01-14 04:02:30,591] [    INFO]\u001b[0m - step_idx: 1909, epoch: 95, batch: 9, avg loss: 1.284880,  ppl: 3.614235 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:02:31,341] [    INFO]\u001b[0m - step_idx: 1919, epoch: 95, batch: 19, avg loss: 1.289581,  ppl: 3.631265 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:02:31,420] [    INFO]\u001b[0m - validation, step_idx: 1919, avg loss: 7.804824,  ppl: 2452.404541\u001b[0m\n",
      "\u001b[32m[2024-01-14 04:02:33,546] [    INFO]\u001b[0m - step_idx: 1929, epoch: 96, batch: 9, avg loss: 1.285716,  ppl: 3.617259 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:02:34,287] [    INFO]\u001b[0m - step_idx: 1939, epoch: 96, batch: 19, avg loss: 1.288007,  ppl: 3.625555 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:02:34,369] [    INFO]\u001b[0m - validation, step_idx: 1939, avg loss: 7.795581,  ppl: 2429.841553\u001b[0m\n",
      "\u001b[32m[2024-01-14 04:02:36,210] [    INFO]\u001b[0m - step_idx: 1949, epoch: 97, batch: 9, avg loss: 1.285099,  ppl: 3.615027 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:02:36,971] [    INFO]\u001b[0m - step_idx: 1959, epoch: 97, batch: 19, avg loss: 1.284314,  ppl: 3.612190 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:02:37,052] [    INFO]\u001b[0m - validation, step_idx: 1959, avg loss: 7.816895,  ppl: 2482.186279\u001b[0m\n",
      "\u001b[32m[2024-01-14 04:02:38,941] [    INFO]\u001b[0m - step_idx: 1969, epoch: 98, batch: 9, avg loss: 1.275698,  ppl: 3.581201 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:02:39,689] [    INFO]\u001b[0m - step_idx: 1979, epoch: 98, batch: 19, avg loss: 1.282762,  ppl: 3.606588 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:02:39,771] [    INFO]\u001b[0m - validation, step_idx: 1979, avg loss: 7.829181,  ppl: 2512.871094\u001b[0m\n",
      "\u001b[32m[2024-01-14 04:02:41,644] [    INFO]\u001b[0m - step_idx: 1989, epoch: 99, batch: 9, avg loss: 1.285986,  ppl: 3.618234 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:02:42,375] [    INFO]\u001b[0m - step_idx: 1999, epoch: 99, batch: 19, avg loss: 1.279545,  ppl: 3.595003 \u001b[0m\n",
      "\u001b[32m[2024-01-14 04:02:42,456] [    INFO]\u001b[0m - validation, step_idx: 1999, avg loss: 7.788840,  ppl: 2413.515869\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "do_train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paddle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
