{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "本notebook使用的是一般的transformer模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1、导入相关包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/setuptools/sandbox.py:13: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources\n",
      "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from functools import partial\n",
    "from pprint import pprint\n",
    "import yaml\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "from attrdict import AttrDict\n",
    "import numpy as np\n",
    "\n",
    "import paddle\n",
    "import paddle.nn as nn\n",
    "from paddle.nn import TransformerDecoder,TransformerDecoderLayer,TransformerEncoder,TransformerEncoderLayer\n",
    "from paddle.nn import functional as F\n",
    "import paddle.distributed as dist\n",
    "from paddle.io import DataLoader,BatchSampler\n",
    "from paddlenlp.data import Vocab, Pad\n",
    "from paddlenlp.datasets import load_dataset\n",
    "from paddlenlp.transformers import TransformerModel, InferTransformerModel, CrossEntropyCriterion, position_encoding_init\n",
    "from paddlenlp.utils.log import logger\n",
    "\n",
    "from helper.utils import post_process_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2、准备数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  中文需要Jieba+BPE，英文需要BPE  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jieba tokenize...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.822 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "source learn-bpe and apply-bpe...\n",
      "no pair has frequency >= 2. Stopping\n",
      "target learn-bpe and apply-bpe...\n",
      "no pair has frequency >= 2. Stopping\n",
      "source get-vocab. if loading pretrained model, use its vocab.\n",
      "target get-vocab. if loading pretrained model, use its vocab.\n",
      "Over.\n"
     ]
    }
   ],
   "source": [
    "# 数据预处理过程，包括jieba分词、bpe分词和词表。\n",
    "!bash ./helper/preprocess.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "构造Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 自定义读取本地数据的方法\n",
    "def read(src_path, tgt_path, is_predict=False):\n",
    "    if is_predict:\n",
    "        with open(src_path, 'r', encoding='utf8') as src_f:\n",
    "            for src_line in src_f.readlines():\n",
    "                src_line = src_line.strip()\n",
    "                if not src_line:\n",
    "                    continue\n",
    "                yield {'src':src_line, 'tgt':''}\n",
    "    else:\n",
    "        with open(src_path, 'r', encoding='utf8') as src_f, open(tgt_path, 'r', encoding='utf8') as tgt_f:\n",
    "            for src_line, tgt_line in zip(src_f.readlines(), tgt_f.readlines()):\n",
    "                src_line = src_line.strip()\n",
    "                if not src_line:\n",
    "                    continue\n",
    "                tgt_line = tgt_line.strip()\n",
    "                if not tgt_line:\n",
    "                    continue\n",
    "                yield {'src':src_line, 'tgt':tgt_line}\n",
    " # 过滤掉长度 ≤min_len或者≥max_len 的数据            \n",
    "def min_max_filer(data, max_len, min_len=0):\n",
    "    # 1 for special tokens.\n",
    "    data_min_len = min(len(data[0]), len(data[1])) + 1\n",
    "    data_max_len = max(len(data[0]), len(data[1])) + 1\n",
    "    return (data_min_len >= min_len) and (data_max_len <= max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 创建训练集、验证集的dataloader\n",
    "def create_data_loader(args):\n",
    "    train_dataset = load_dataset(read, src_path=args.training_file.split(',')[0], tgt_path=args.training_file.split(',')[1], lazy=False)\n",
    "    dev_dataset = load_dataset(read, src_path=args.validation_file.split(',')[0], tgt_path=args.validation_file.split(',')[1], lazy=False)\n",
    "\n",
    "    src_vocab = Vocab.load_vocabulary(\n",
    "        args.src_vocab_fpath,\n",
    "        bos_token=args.special_token[0],\n",
    "        eos_token=args.special_token[1],\n",
    "        unk_token=args.special_token[2])\n",
    "    trg_vocab = Vocab.load_vocabulary(\n",
    "        args.trg_vocab_fpath,\n",
    "        bos_token=args.special_token[0],\n",
    "        eos_token=args.special_token[1],\n",
    "        unk_token=args.special_token[2])\n",
    "\n",
    "    padding_vocab = (\n",
    "        lambda x: (x + args.pad_factor - 1) // args.pad_factor * args.pad_factor\n",
    "    )\n",
    "    args.src_vocab_size = padding_vocab(len(src_vocab))\n",
    "    args.trg_vocab_size = padding_vocab(len(trg_vocab))\n",
    "\n",
    "    def convert_samples(sample):\n",
    "        source = sample['src'].split()\n",
    "        target = sample['tgt'].split()\n",
    "\n",
    "        source = src_vocab.to_indices(source)\n",
    "        target = trg_vocab.to_indices(target)\n",
    "\n",
    "        return source, target\n",
    "\n",
    "    # 训练集dataloader和验证集dataloader\n",
    "    data_loaders = []\n",
    "    for i, dataset in enumerate([train_dataset, dev_dataset]):\n",
    "        dataset = dataset.map(convert_samples, lazy=False).filter(\n",
    "            partial(min_max_filer, max_len=args.max_length))\n",
    "\n",
    "        # BatchSampler: https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/io/BatchSampler_cn.html\n",
    "        batch_sampler = BatchSampler(dataset,batch_size=args.batch_size, shuffle=True,drop_last=False)\n",
    "        \n",
    "        # DataLoader: https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/io/DataLoader_cn.html\n",
    "        data_loader = DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_sampler=batch_sampler,\n",
    "            collate_fn=partial(\n",
    "                prepare_train_input,\n",
    "                bos_idx=args.bos_idx,\n",
    "                eos_idx=args.eos_idx,\n",
    "                pad_idx=args.bos_idx),\n",
    "                num_workers=0,\n",
    "                return_list=True)\n",
    "        data_loaders.append(data_loader)\n",
    "\n",
    "    return data_loaders\n",
    "\n",
    "\n",
    "def prepare_train_input(insts, bos_idx, eos_idx, pad_idx):\n",
    "    \"\"\"\n",
    "    Put all padded data needed by training into a list.\n",
    "    \"\"\"\n",
    "    word_pad = Pad(pad_idx)\n",
    "    src_word = word_pad([inst[0] + [eos_idx] for inst in insts])\n",
    "    trg_word = word_pad([[bos_idx] + inst[1] for inst in insts])\n",
    "    lbl_word = np.expand_dims(\n",
    "        word_pad([inst[1] + [eos_idx] for inst in insts]), axis=2)\n",
    "\n",
    "    data_inputs = [src_word, trg_word, lbl_word]\n",
    "\n",
    "    return data_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 创建测试集的dataloader，原理步骤同上（创建训练集、验证集的dataloader）\n",
    "def create_infer_loader(args):\n",
    "    dataset = load_dataset(read, src_path=args.predict_file, tgt_path=None, is_predict=True, lazy=False)\n",
    "\n",
    "    src_vocab = Vocab.load_vocabulary(\n",
    "        args.src_vocab_fpath,\n",
    "        bos_token=args.special_token[0],\n",
    "        eos_token=args.special_token[1],\n",
    "        unk_token=args.special_token[2])\n",
    "    trg_vocab = Vocab.load_vocabulary(\n",
    "        args.trg_vocab_fpath,\n",
    "        bos_token=args.special_token[0],\n",
    "        eos_token=args.special_token[1],\n",
    "        unk_token=args.special_token[2])\n",
    "\n",
    "    padding_vocab = (\n",
    "        lambda x: (x + args.pad_factor - 1) // args.pad_factor * args.pad_factor\n",
    "    )\n",
    "    args.src_vocab_size = padding_vocab(len(src_vocab))\n",
    "    args.trg_vocab_size = padding_vocab(len(trg_vocab))\n",
    "\n",
    "    def convert_samples(sample):\n",
    "        source = sample['src'].split()\n",
    "        target = sample['tgt'].split()\n",
    "\n",
    "        source = src_vocab.to_indices(source)\n",
    "        target = trg_vocab.to_indices(target)\n",
    "\n",
    "        return source, target\n",
    "\n",
    "    dataset = dataset.map(convert_samples, lazy=False)\n",
    "\n",
    "    # BatchSampler: https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/io/BatchSampler_cn.html\n",
    "    batch_sampler = BatchSampler(dataset,batch_size=args.infer_batch_size,drop_last=False)\n",
    "    \n",
    "    # DataLoader: https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/io/DataLoader_cn.html\n",
    "    data_loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_sampler=batch_sampler,\n",
    "        collate_fn=partial(\n",
    "            prepare_infer_input,\n",
    "            bos_idx=args.bos_idx,\n",
    "            eos_idx=args.eos_idx,\n",
    "            pad_idx=args.bos_idx),\n",
    "            num_workers=0,\n",
    "            return_list=True)\n",
    "    return data_loader, trg_vocab.to_tokens\n",
    "\n",
    "def prepare_infer_input(insts, bos_idx, eos_idx, pad_idx):\n",
    "    \"\"\"\n",
    "    Put all padded data needed by beam search decoder into a list.\n",
    "    \"\"\"\n",
    "    word_pad = Pad(pad_idx)\n",
    "    src_word = word_pad([inst[0] + [eos_idx] for inst in insts])\n",
    "\n",
    "    return [src_word, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3、准备网络模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "搭建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 网络基本模块\n",
    "class WordEmbedding(nn.Layer):\n",
    "    def __init__(self, vocab_size, emb_dim, bos_id=0):\n",
    "        super(WordEmbedding, self).__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "        self.word_embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=emb_dim,\n",
    "            padding_idx=bos_id,\n",
    "            weight_attr=paddle.ParamAttr(initializer=nn.initializer.Normal(0.0, emb_dim ** (-0.5))),\n",
    "        )\n",
    "\n",
    "    def forward(self, word):\n",
    "        word_emb = self.emb_dim**0.5 * self.word_embedding(word)\n",
    "        return word_emb\n",
    "\n",
    "\n",
    "class PositionalEmbedding(nn.Layer):\n",
    "    def __init__(self, emb_dim, max_length):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "        self.pos_encoder = nn.Embedding(\n",
    "            num_embeddings=max_length,\n",
    "            embedding_dim=self.emb_dim,\n",
    "            weight_attr=paddle.ParamAttr(\n",
    "                initializer=paddle.nn.initializer.Assign(position_encoding_init(max_length, self.emb_dim))\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, pos):\n",
    "        pos_emb = self.pos_encoder(pos)\n",
    "        pos_emb.stop_gradient = True\n",
    "        return pos_emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Layer):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        src_vocab_size (int):\n",
    "            The size of source vocabulary.\n",
    "        trg_vocab_size (int):\n",
    "            The size of target vocabulary.\n",
    "        max_length (int):\n",
    "            The maximum length of input sequences.\n",
    "        num_encoder_layers (int):\n",
    "            The number of sub-layers to be stacked in the encoder.\n",
    "        num_decoder_layers (int):\n",
    "            The number of sub-layers to be stacked in the decoder.\n",
    "        n_head (int):\n",
    "            The number of head used in multi-head attention.\n",
    "        d_model (int):\n",
    "            The dimension for word embeddings, which is also the last dimension of\n",
    "            the input and output of multi-head attention, position-wise feed-forward\n",
    "            networks, encoder and decoder.\n",
    "        d_inner_hid (int):\n",
    "            Size of the hidden layer in position-wise feed-forward networks.\n",
    "        dropout (float):\n",
    "            Dropout rates. Used for pre-process, activation and inside attention.\n",
    "        weight_sharing (bool):\n",
    "            Whether to use weight sharing.\n",
    "        attn_dropout (float):\n",
    "            The dropout probability used in MHA to drop some attention target.\n",
    "            If None, use the value of dropout. Defaults to None.\n",
    "        act_dropout (float):\n",
    "            The dropout probability used after FFN activation. If None, use\n",
    "            the value of dropout. Defaults to None.\n",
    "        bos_id (int, optional):\n",
    "            The start token id and also be used as padding id. Defaults to 0.\n",
    "        eos_id (int, optional):\n",
    "            The end token id. Defaults to 1.\n",
    "        pad_id (int, optional):\n",
    "            The pad token id. Defaults to None. If it's None, the bos_id will be used as pad_id.\n",
    "        activation (str, optional):\n",
    "            The activation used in FFN. Defaults to \"relu\".\n",
    "        normalize_before (bool, optional):\n",
    "            Whether to apply pre-normalization. Defaults to True.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size,\n",
    "        trg_vocab_size,\n",
    "        max_length,\n",
    "        num_encoder_layers,\n",
    "        num_decoder_layers,\n",
    "        n_head,\n",
    "        d_model,\n",
    "        d_inner_hid,\n",
    "        dropout,\n",
    "        weight_sharing,\n",
    "        attn_dropout=None,\n",
    "        act_dropout=None,\n",
    "        bos_id=0,\n",
    "        eos_id=1,\n",
    "        pad_id=None,\n",
    "        activation=\"relu\",\n",
    "        normalize_before=True,\n",
    "    ):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.trg_vocab_size = trg_vocab_size\n",
    "        self.emb_dim = d_model\n",
    "        self.bos_id = bos_id\n",
    "        self.eos_id = eos_id\n",
    "        self.pad_id = pad_id if pad_id is not None else self.bos_id\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.src_word_embedding = WordEmbedding(vocab_size=src_vocab_size, emb_dim=d_model, bos_id=self.pad_id)\n",
    "        self.src_pos_embedding = PositionalEmbedding(emb_dim=d_model, max_length=max_length)\n",
    "        if weight_sharing:\n",
    "            assert (\n",
    "                src_vocab_size == trg_vocab_size\n",
    "            ), \"Vocabularies in source and target should be same for weight sharing.\"\n",
    "            self.trg_word_embedding = self.src_word_embedding\n",
    "            self.trg_pos_embedding = self.src_pos_embedding\n",
    "        else:\n",
    "            self.trg_word_embedding = WordEmbedding(vocab_size=trg_vocab_size, emb_dim=d_model, bos_id=self.pad_id)\n",
    "            self.trg_pos_embedding = PositionalEmbedding(emb_dim=d_model, max_length=max_length)\n",
    "\n",
    "        if not normalize_before:\n",
    "            encoder_layer = TransformerEncoderLayer(\n",
    "                d_model=d_model,\n",
    "                nhead=n_head,\n",
    "                dim_feedforward=d_inner_hid,\n",
    "                dropout=dropout,\n",
    "                activation=activation,\n",
    "                attn_dropout=attn_dropout,\n",
    "                act_dropout=act_dropout,\n",
    "                normalize_before=normalize_before,\n",
    "            )\n",
    "            encoder_with_post_norm = TransformerEncoder(encoder_layer, num_encoder_layers)\n",
    "\n",
    "            decoder_layer = TransformerDecoderLayer(\n",
    "                d_model=d_model,\n",
    "                nhead=n_head,\n",
    "                dim_feedforward=d_inner_hid,\n",
    "                dropout=dropout,\n",
    "                activation=activation,\n",
    "                attn_dropout=attn_dropout,\n",
    "                act_dropout=act_dropout,\n",
    "                normalize_before=normalize_before,\n",
    "            )\n",
    "            decoder_with_post_norm = TransformerDecoder(decoder_layer, num_decoder_layers)\n",
    "\n",
    "        self.transformer = paddle.nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_head,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=d_inner_hid,\n",
    "            dropout=dropout,\n",
    "            attn_dropout=attn_dropout,\n",
    "            act_dropout=act_dropout,\n",
    "            activation=activation,\n",
    "            normalize_before=normalize_before,\n",
    "            custom_encoder=None if normalize_before else encoder_with_post_norm,\n",
    "            custom_decoder=None if normalize_before else decoder_with_post_norm,\n",
    "        )\n",
    "\n",
    "        if weight_sharing:\n",
    "            self.linear = lambda x: paddle.matmul(\n",
    "                x=x, y=self.trg_word_embedding.word_embedding.weight, transpose_y=True\n",
    "            )\n",
    "        else:\n",
    "            self.linear = nn.Linear(in_features=d_model, out_features=trg_vocab_size, bias_attr=False)\n",
    "\n",
    "    def forward(self, src_word, trg_word):\n",
    "        src_max_len = paddle.shape(src_word)[-1]\n",
    "        trg_max_len = paddle.shape(trg_word)[-1]\n",
    "        src_slf_attn_bias = (\n",
    "            paddle.cast(src_word == self.pad_id, dtype=paddle.get_default_dtype()).unsqueeze([1, 2]) * -1e4\n",
    "        )\n",
    "        src_slf_attn_bias.stop_gradient = True\n",
    "        trg_slf_attn_bias = self.transformer.generate_square_subsequent_mask(trg_max_len)\n",
    "        trg_slf_attn_bias.stop_gradient = True\n",
    "        trg_src_attn_bias = src_slf_attn_bias\n",
    "        src_pos = paddle.cast(src_word != self.pad_id, dtype=src_word.dtype) * paddle.arange(\n",
    "            start=0, end=src_max_len, dtype=src_word.dtype\n",
    "        )\n",
    "        trg_pos = paddle.cast(trg_word != self.pad_id, dtype=src_word.dtype) * paddle.arange(\n",
    "            start=0, end=trg_max_len, dtype=trg_word.dtype\n",
    "        )\n",
    "\n",
    "        with paddle.static.amp.fp16_guard():\n",
    "            src_emb = self.src_word_embedding(src_word)\n",
    "            src_pos_emb = self.src_pos_embedding(src_pos)\n",
    "            src_emb = src_emb + src_pos_emb\n",
    "            enc_input = F.dropout(src_emb, p=self.dropout, training=self.training) if self.dropout else src_emb\n",
    "\n",
    "            trg_emb = self.trg_word_embedding(trg_word)\n",
    "            trg_pos_emb = self.trg_pos_embedding(trg_pos)\n",
    "            trg_emb = trg_emb + trg_pos_emb\n",
    "            dec_input = F.dropout(trg_emb, p=self.dropout, training=self.training) if self.dropout else trg_emb\n",
    "\n",
    "            dec_output = self.transformer(\n",
    "                enc_input,\n",
    "                dec_input,\n",
    "                src_mask=src_slf_attn_bias,\n",
    "                tgt_mask=trg_slf_attn_bias,\n",
    "                memory_mask=trg_src_attn_bias,\n",
    "            )\n",
    "\n",
    "            predict = self.linear(dec_output)\n",
    "\n",
    "        return predict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1229 08:10:04.669833 73320 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.2, Runtime API Version: 11.7\n",
      "W1229 08:10:04.677749 73320 gpu_resources.cc:91] device: 0, cuDNN Version: 8.5.\n"
     ]
    }
   ],
   "source": [
    "model = TransformerModel(\n",
    "    src_vocab_size=10000,\n",
    "    trg_vocab_size=10000,\n",
    "    max_length=256 + 1,\n",
    "    num_encoder_layers=6,\n",
    "    num_decoder_layers=6,\n",
    "    n_head=8,\n",
    "    d_model=512,\n",
    "    d_inner_hid=2048,\n",
    "    dropout=0.1,\n",
    "    weight_sharing=False,\n",
    "    bos_id=0,\n",
    "    eos_id=1)\n",
    "# print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/fb181b57c2d347b884502d5d11d8c61e918ee803069d4e26bcf4c6533cf948c6\" width=\"1000\" height=\"500\" ></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下载预训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over.\n"
     ]
    }
   ],
   "source": [
    "# 下载预训练模型\n",
    "!bash ./helper/get_data_and_model.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4、训练模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化\n",
    "from visualdl import LogWriter\n",
    "logwriter = LogWriter(logdir='./visualdl/transformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def do_train(args):\n",
    "    if args.use_gpu:\n",
    "        place = \"gpu\"\n",
    "    else:\n",
    "        place = \"cpu\"\n",
    "    paddle.set_device(place)\n",
    "    # Set seed for CE\n",
    "    random_seed = eval(str(args.random_seed))\n",
    "    if random_seed is not None:\n",
    "        paddle.seed(random_seed)\n",
    "\n",
    "    # Define data loader\n",
    "    (train_loader), (eval_loader) = create_data_loader(args)\n",
    "\n",
    "    # Define model\n",
    "    transformer = TransformerModel( # 用于训练\n",
    "        src_vocab_size=args.src_vocab_size,\n",
    "        trg_vocab_size=args.trg_vocab_size,\n",
    "        max_length=args.max_length + 1,\n",
    "        num_encoder_layers=args.n_layer,\n",
    "        num_decoder_layers=args.n_layer,\n",
    "        n_head=args.n_head,\n",
    "        d_model=args.d_model,\n",
    "        d_inner_hid=args.d_inner_hid,\n",
    "        dropout=args.dropout,\n",
    "        weight_sharing=args.weight_sharing,\n",
    "        bos_id=args.bos_idx,\n",
    "        eos_id=args.eos_idx)\n",
    "\n",
    "    # Define loss\n",
    "    criterion = CrossEntropyCriterion(args.label_smooth_eps, args.bos_idx)\n",
    "\n",
    "    scheduler = paddle.optimizer.lr.NoamDecay(\n",
    "        args.d_model, args.warmup_steps, args.learning_rate, last_epoch=0)\n",
    "\n",
    "    # Define optimizer\n",
    "    optimizer = paddle.optimizer.Adam(\n",
    "        learning_rate=scheduler,\n",
    "        beta1=args.beta1,\n",
    "        beta2=args.beta2,\n",
    "        epsilon=float(args.eps),\n",
    "        parameters=transformer.parameters())\n",
    "\n",
    "    step_idx = 0\n",
    "\n",
    "    # Train loop\n",
    "    for pass_id in range(args.epoch):\n",
    "        batch_id = 0\n",
    "        for input_data in train_loader:\n",
    "\n",
    "            (src_word, trg_word, lbl_word) = input_data\n",
    "\n",
    "            logits = transformer(src_word=src_word, trg_word=trg_word)\n",
    "\n",
    "            sum_cost, avg_cost, token_num = criterion(logits, lbl_word)\n",
    "            \n",
    "            # 计算梯度\n",
    "            avg_cost.backward() \n",
    "            # 更新参数\n",
    "            optimizer.step() \n",
    "            # 梯度清零\n",
    "            optimizer.clear_grad() \n",
    "\n",
    "            if (step_idx + 1) % args.print_step == 0 or step_idx == 0:\n",
    "                total_avg_cost = avg_cost.numpy()\n",
    "                logger.info(\n",
    "                    \"step_idx: %d, epoch: %d, batch: %d, avg loss: %f, \"\n",
    "                    \" ppl: %f \" %\n",
    "                    (step_idx, pass_id, batch_id, total_avg_cost,\n",
    "                        np.exp([min(total_avg_cost, 100)])))\n",
    "                logwriter.add_scalar(\"train_loss\", value=total_avg_cost, step=step_idx+pass_id*(args.batch_size))\n",
    "                logwriter.add_scalar(\"train_perplexity\", value=np.exp([min(total_avg_cost, 100)]), step=step_idx+pass_id*(args.batch_size))\n",
    "\n",
    "            if (step_idx + 1) % args.save_step == 0:\n",
    "                # Validation\n",
    "                transformer.eval()\n",
    "                total_sum_cost = 0\n",
    "                total_token_num = 0\n",
    "                with paddle.no_grad():\n",
    "                    for input_data in eval_loader:\n",
    "                        (src_word, trg_word, lbl_word) = input_data\n",
    "                        logits = transformer(\n",
    "                            src_word=src_word, trg_word=trg_word)\n",
    "                        sum_cost, avg_cost, token_num = criterion(logits,\n",
    "                                                                  lbl_word)\n",
    "                        total_sum_cost += sum_cost.numpy()\n",
    "                        total_token_num += token_num.numpy()\n",
    "                        total_avg_cost = total_sum_cost / total_token_num\n",
    "                    logger.info(\"validation, step_idx: %d, avg loss: %f, \"\n",
    "                                \" ppl: %f\" %\n",
    "                                (step_idx, total_avg_cost,\n",
    "                                 np.exp([min(total_avg_cost, 100)])))\n",
    "                    logwriter.add_scalar(\"valid_loss\", value=total_avg_cost, step=step_idx+pass_id*(args.batch_size))\n",
    "                    logwriter.add_scalar(\"valid_perplexity\", value=np.exp([min(total_avg_cost, 100)]), step=step_idx+pass_id*(args.batch_size))\n",
    "                transformer.train()\n",
    "\n",
    "                if args.save_model:\n",
    "                    model_dir = os.path.join(args.save_model,\n",
    "                                             \"step_\" + str(step_idx))\n",
    "                    if not os.path.exists(model_dir):\n",
    "                        os.makedirs(model_dir)\n",
    "                    paddle.save(transformer.state_dict(),\n",
    "                                os.path.join(model_dir, \"transformer.pdparams\"))\n",
    "                    paddle.save(optimizer.state_dict(),\n",
    "                                os.path.join(model_dir, \"transformer.pdopt\"))\n",
    "            batch_id += 1\n",
    "            step_idx += 1\n",
    "            scheduler.step()\n",
    "\n",
    "\n",
    "    if args.save_model:\n",
    "        model_dir = os.path.join(args.save_model, \"step_final\")\n",
    "        if not os.path.exists(model_dir):\n",
    "            os.makedirs(model_dir)\n",
    "        paddle.save(transformer.state_dict(),\n",
    "                    os.path.join(model_dir, \"transformer.pdparams\"))\n",
    "        paddle.save(optimizer.state_dict(),\n",
    "                    os.path.join(model_dir, \"transformer.pdopt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 50,\n",
      " 'beam_size': 5,\n",
      " 'beta1': 0.9,\n",
      " 'beta2': 0.997,\n",
      " 'bos_idx': 0,\n",
      " 'd_inner_hid': 2048,\n",
      " 'd_model': 512,\n",
      " 'dropout': 0.1,\n",
      " 'eos_idx': 1,\n",
      " 'epoch': 5,\n",
      " 'eps': '1e-9',\n",
      " 'infer_batch_size': 50,\n",
      " 'init_from_params': 'trained_models/CWMT2021_step_345000/',\n",
      " 'label_smooth_eps': 0.1,\n",
      " 'learning_rate': 2.0,\n",
      " 'max_length': 256,\n",
      " 'max_out_len': 256,\n",
      " 'n_best': 1,\n",
      " 'n_head': 8,\n",
      " 'n_layer': 6,\n",
      " 'output_file': './data/train_dev_test/predict.txt',\n",
      " 'pad_factor': 8,\n",
      " 'predict_file': './data/train_dev_test/ccmt2019-news.zh2en.source_cut.txt',\n",
      " 'print_step': 10,\n",
      " 'random_seed': 'None',\n",
      " 'save_model': 'model/transformer',\n",
      " 'save_step': 20,\n",
      " 'special_token': ['<s>', '<e>', '<unk>'],\n",
      " 'src_vocab_fpath': './data/train_dev_test/vocab.ch.src',\n",
      " 'src_vocab_size': 10000,\n",
      " 'training_file': './data/train_dev_test/train.ch.bpe,./data/train_dev_test/train.en.bpe',\n",
      " 'trg_vocab_fpath': './data/train_dev_test/vocab.en.tgt',\n",
      " 'trg_vocab_size': 10000,\n",
      " 'unk_idx': 2,\n",
      " 'use_gpu': True,\n",
      " 'validation_file': './data/train_dev_test/dev.ch.bpe,./data/train_dev_test/dev.en.bpe',\n",
      " 'warmup_steps': 8000,\n",
      " 'weight_sharing': False}\n"
     ]
    }
   ],
   "source": [
    "# 读入参数\n",
    "yaml_file = './helper/transformer.base.yaml'\n",
    "with open(yaml_file, 'rt') as f:\n",
    "    args = AttrDict(yaml.safe_load(f))\n",
    "    pprint(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-12-29 08:10:08,575] [    INFO]\u001b[0m - step_idx: 0, epoch: 0, batch: 0, avg loss: 10.515485,  ppl: 36882.218750 \u001b[0m\n",
      "\u001b[32m[2023-12-29 08:10:09,026] [    INFO]\u001b[0m - step_idx: 9, epoch: 0, batch: 9, avg loss: 10.504098,  ppl: 36464.628906 \u001b[0m\n",
      "\u001b[32m[2023-12-29 08:10:09,455] [    INFO]\u001b[0m - step_idx: 19, epoch: 0, batch: 19, avg loss: 10.464189,  ppl: 35038.000000 \u001b[0m\n",
      "\u001b[32m[2023-12-29 08:10:09,502] [    INFO]\u001b[0m - validation, step_idx: 19, avg loss: 10.472076,  ppl: 35315.468750\u001b[0m\n",
      "\u001b[32m[2023-12-29 08:10:13,117] [    INFO]\u001b[0m - step_idx: 29, epoch: 1, batch: 9, avg loss: 10.407406,  ppl: 33103.882812 \u001b[0m\n",
      "\u001b[32m[2023-12-29 08:10:13,609] [    INFO]\u001b[0m - step_idx: 39, epoch: 1, batch: 19, avg loss: 10.341732,  ppl: 30999.675781 \u001b[0m\n",
      "\u001b[32m[2023-12-29 08:10:13,658] [    INFO]\u001b[0m - validation, step_idx: 39, avg loss: 10.389604,  ppl: 32519.775391\u001b[0m\n",
      "\u001b[32m[2023-12-29 08:10:16,296] [    INFO]\u001b[0m - step_idx: 49, epoch: 2, batch: 9, avg loss: 10.278871,  ppl: 29110.976562 \u001b[0m\n",
      "\u001b[32m[2023-12-29 08:10:16,754] [    INFO]\u001b[0m - step_idx: 59, epoch: 2, batch: 19, avg loss: 10.196178,  ppl: 26800.570312 \u001b[0m\n",
      "\u001b[32m[2023-12-29 08:10:16,800] [    INFO]\u001b[0m - validation, step_idx: 59, avg loss: 10.291049,  ppl: 29467.669922\u001b[0m\n",
      "\u001b[32m[2023-12-29 08:10:19,446] [    INFO]\u001b[0m - step_idx: 69, epoch: 3, batch: 9, avg loss: 10.154450,  ppl: 25705.246094 \u001b[0m\n",
      "\u001b[32m[2023-12-29 08:10:19,877] [    INFO]\u001b[0m - step_idx: 79, epoch: 3, batch: 19, avg loss: 10.054660,  ppl: 23263.939453 \u001b[0m\n",
      "\u001b[32m[2023-12-29 08:10:19,924] [    INFO]\u001b[0m - validation, step_idx: 79, avg loss: 10.168482,  ppl: 26068.468750\u001b[0m\n",
      "\u001b[32m[2023-12-29 08:10:22,514] [    INFO]\u001b[0m - step_idx: 89, epoch: 4, batch: 9, avg loss: 9.994656,  ppl: 21909.062500 \u001b[0m\n",
      "\u001b[32m[2023-12-29 08:10:22,949] [    INFO]\u001b[0m - step_idx: 99, epoch: 4, batch: 19, avg loss: 9.925208,  ppl: 20439.164062 \u001b[0m\n",
      "\u001b[32m[2023-12-29 08:10:22,994] [    INFO]\u001b[0m - validation, step_idx: 99, avg loss: 10.016716,  ppl: 22397.753906\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "do_train(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 5、预测和评估"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def do_predict(args):\n",
    "    if args.use_gpu:\n",
    "        place = \"gpu\"\n",
    "    else:\n",
    "        place = \"cpu\"\n",
    "    paddle.set_device(place)\n",
    "\n",
    "    # Define data loader\n",
    "    test_loader, to_tokens = create_infer_loader(args)\n",
    "\n",
    "    # Define model\n",
    "    transformer = InferTransformerModel( # 用于生成\n",
    "        src_vocab_size=args.src_vocab_size,\n",
    "        trg_vocab_size=args.trg_vocab_size,\n",
    "        max_length=args.max_length + 1,\n",
    "        num_encoder_layers=args.n_layer,\n",
    "        num_decoder_layers=args.n_layer,\n",
    "        n_head=args.n_head,\n",
    "        d_model=args.d_model,\n",
    "        d_inner_hid=args.d_inner_hid,\n",
    "        dropout=args.dropout,\n",
    "        weight_sharing=args.weight_sharing,\n",
    "        bos_id=args.bos_idx,\n",
    "        eos_id=args.eos_idx,\n",
    "        beam_size=args.beam_size,\n",
    "        max_out_len=args.max_out_len)\n",
    "\n",
    "    # Load the trained model\n",
    "    assert args.init_from_params, (\n",
    "        \"Please set init_from_params to load the infer model.\")\n",
    "\n",
    "    model_dict = paddle.load(\n",
    "        os.path.join(args.init_from_params, \"transformer.pdparams\"))\n",
    "\n",
    "    # To avoid a longer length than training, reset the size of position\n",
    "    # encoding to max_length\n",
    "    model_dict[\"encoder.pos_encoder.weight\"] = position_encoding_init(\n",
    "        args.max_length + 1, args.d_model)\n",
    "    model_dict[\"decoder.pos_encoder.weight\"] = position_encoding_init(\n",
    "        args.max_length + 1, args.d_model)\n",
    "    transformer.load_dict(model_dict)\n",
    "\n",
    "    # Set evaluate mode\n",
    "    transformer.eval()\n",
    "\n",
    "    f = open(args.output_file, \"w\")\n",
    "    with paddle.no_grad():\n",
    "        for (src_word, ) in test_loader:\n",
    "            finished_seq = transformer(src_word=src_word)\n",
    "            finished_seq = finished_seq.numpy().transpose([0, 2, 1])\n",
    "            for ins in finished_seq:\n",
    "                for beam_idx, beam in enumerate(ins):\n",
    "                    if beam_idx >= args.n_best:\n",
    "                        break\n",
    "                    id_list = post_process_seq(beam, args.bos_idx, args.eos_idx)\n",
    "                    word_list = to_tokens(id_list)\n",
    "                    sequence = \" \".join(word_list) + \"\\n\"\n",
    "                    f.write(sequence)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "do_predict(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "模型评估"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 预测结果中每行输出是对应行输入的得分最高的翻译，对于使用 BPE 的数据，预测出的翻译结果也将是 BPE 表示的数据，要还原成原始的数据（这里指 tokenize 后的数据）才能进行正确的评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU = 28.73, 65.5/39.6/25.1/16.0 (BP=0.898, ratio=0.903, hyp_len=20687, ref_len=22902)\n",
      "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n"
     ]
    }
   ],
   "source": [
    "# 还原 predict.txt 中的预测结果为 tokenize 后的数据\n",
    "! sed -r 's/(@@ )|(@@ ?$)//g' ./data/train_dev_test/predict.txt > ./data/train_dev_test/predict.tok.txt\n",
    "# BLEU评估工具来源于 https://github.com/moses-smt/mosesdecoder.git\n",
    "# 计算multi-bleu\n",
    "! perl ./helper/mosesdecoder/scripts/generic/multi-bleu.perl ./data/train_dev_test/ccmt2019-news.zh2en.ref*.txt < ./data/train_dev_test/predict.tok.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paddle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
