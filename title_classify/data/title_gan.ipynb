{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本notebook的gan模型的生成器为LSTM网络，判别器是CNN网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1、导入相关包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T05:27:24.351652Z",
     "iopub.status.busy": "2022-07-20T05:27:24.350929Z",
     "iopub.status.idle": "2022-07-20T05:27:26.428062Z",
     "shell.execute_reply": "2022-07-20T05:27:26.426855Z",
     "shell.execute_reply.started": "2022-07-20T05:27:24.351608Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/setuptools/sandbox.py:13: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources\n",
      "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "# 数据科学包\n",
    "import random                      # 随机切分数据集\n",
    "import numpy as np                 # 常用数据科学包\n",
    "import pandas as pd              # 图像读取\n",
    "import matplotlib.pyplot as plt    # 代码中快速验证\n",
    "\n",
    "# 深度学习包\n",
    "import paddle\n",
    "import paddle.vision.transforms as tf      # 数据增强\n",
    "from paddle.io import Dataset, DataLoader  # 定义数据集\n",
    "import paddle.nn as nn                     # 网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、准备数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用pandas读取数据集\n",
    "train_data = pd.read_table('./data/train.txt', sep='\\t',header=None)  # 训练集\n",
    "dev_data = pd.read_table('./data/dev.txt', sep='\\t',header=None)      # 验证集\n",
    "test_data = pd.read_table('./data/test.txt', sep='\\t',header=None)    # 测试集\n",
    "\n",
    "# 由于数据集存放时无列名，因此手动添加列名便于对数据进行更好处理\n",
    "train_data.columns = [\"text\",'label']\n",
    "dev_data.columns = [\"text\",'label']\n",
    "test_data.columns = [\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>网易第三季度业绩低于分析师预期</td>\n",
       "      <td>科技</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>巴萨1年前地狱重现这次却是天堂 再赴魔鬼客场必翻盘</td>\n",
       "      <td>体育</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>美国称支持向朝鲜提供紧急人道主义援助</td>\n",
       "      <td>时政</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>增资交银康联 交行夺参股险商首单</td>\n",
       "      <td>股票</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>午盘：原材料板块领涨大盘</td>\n",
       "      <td>股票</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752466</th>\n",
       "      <td>天津女排奇迹之源竟在场边 他是五冠王真正核心</td>\n",
       "      <td>体育</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752467</th>\n",
       "      <td>北电网络专利拍卖推迟：可能分拆6部分拍卖</td>\n",
       "      <td>科技</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752468</th>\n",
       "      <td>Spirit AeroSystems债券发行价确定</td>\n",
       "      <td>股票</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752469</th>\n",
       "      <td>陆慧明必发火线：法兰克福无胜 曼联国米顺利过关</td>\n",
       "      <td>彩票</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752470</th>\n",
       "      <td>首破万元 索尼46寸全新LED液晶特价促</td>\n",
       "      <td>科技</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>752471 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             text label\n",
       "0                 网易第三季度业绩低于分析师预期    科技\n",
       "1       巴萨1年前地狱重现这次却是天堂 再赴魔鬼客场必翻盘    体育\n",
       "2              美国称支持向朝鲜提供紧急人道主义援助    时政\n",
       "3                增资交银康联 交行夺参股险商首单    股票\n",
       "4                    午盘：原材料板块领涨大盘    股票\n",
       "...                           ...   ...\n",
       "752466     天津女排奇迹之源竟在场边 他是五冠王真正核心    体育\n",
       "752467       北电网络专利拍卖推迟：可能分拆6部分拍卖    科技\n",
       "752468  Spirit AeroSystems债券发行价确定    股票\n",
       "752469    陆慧明必发火线：法兰克福无胜 曼联国米顺利过关    彩票\n",
       "752470       首破万元 索尼46寸全新LED液晶特价促    科技\n",
       "\n",
       "[752471 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "处理数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'科技': 0, '体育': 1, '时政': 2, '股票': 3, '娱乐': 4, '教育': 5, '家居': 6, '财经': 7, '房产': 8, '社会': 9, '游戏': 10, '彩票': 11, '星座': 12, '时尚': 13}\n"
     ]
    }
   ],
   "source": [
    "# 定义要进行分类的类别\n",
    "label_list=list(train_data.label.unique())\n",
    "label_word2num = { \n",
    "    label_text : idx for idx, label_text in enumerate(label_list)\n",
    "}\n",
    "label_num2word = { \n",
    "    idx : label_text for idx, label_text in enumerate(label_list)\n",
    "}\n",
    "print(label_word2num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>网易第三季度业绩低于分析师预期</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>巴萨1年前地狱重现这次却是天堂 再赴魔鬼客场必翻盘</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>美国称支持向朝鲜提供紧急人道主义援助</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>增资交银康联 交行夺参股险商首单</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>午盘：原材料板块领涨大盘</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752466</th>\n",
       "      <td>天津女排奇迹之源竟在场边 他是五冠王真正核心</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752467</th>\n",
       "      <td>北电网络专利拍卖推迟：可能分拆6部分拍卖</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752468</th>\n",
       "      <td>Spirit AeroSystems债券发行价确定</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752469</th>\n",
       "      <td>陆慧明必发火线：法兰克福无胜 曼联国米顺利过关</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752470</th>\n",
       "      <td>首破万元 索尼46寸全新LED液晶特价促</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>752471 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             text label\n",
       "0                 网易第三季度业绩低于分析师预期     0\n",
       "1       巴萨1年前地狱重现这次却是天堂 再赴魔鬼客场必翻盘     1\n",
       "2              美国称支持向朝鲜提供紧急人道主义援助     2\n",
       "3                增资交银康联 交行夺参股险商首单     3\n",
       "4                    午盘：原材料板块领涨大盘     3\n",
       "...                           ...   ...\n",
       "752466     天津女排奇迹之源竟在场边 他是五冠王真正核心     1\n",
       "752467       北电网络专利拍卖推迟：可能分拆6部分拍卖     0\n",
       "752468  Spirit AeroSystems债券发行价确定     3\n",
       "752469    陆慧明必发火线：法兰克福无胜 曼联国米顺利过关    11\n",
       "752470       首破万元 索尼46寸全新LED液晶特价促     0\n",
       "\n",
       "[752471 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 标签：文本转数字\n",
    "train_data.iloc[:, 1] = train_data.iloc[:, 1].map(label_word2num)\n",
    "dev_data.iloc[:, 1] = dev_data.iloc[:, 1].map(label_word2num)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef build_vocab(text_list):\\n    vocab = {\"<unk>\": 0}  # 添加一个特殊的索引，用于表示未知词\\n\\n    # 遍历所有句子，构建词汇表\\n    for text in text_list:\\n        word_list = jieba.lcut(text)\\n        for word in word_list:\\n            if word not in vocab:\\n                vocab[word] = len(vocab)  # 将每个词映射到唯一的整数索引\\n\\n    return vocab\\n\\nvocabulary = build_vocab(train_data[\\'text\\'].tolist() + dev_data[\\'text\\'].tolist())\\n# 保存词汇表到 JSON 文件, 下次可以直接使用\\nwith open(\\'vocabulary.json\\', \\'w\\', encoding=\\'utf-8\\') as f:\\n    json.dump(vocabulary, f, ensure_ascii=False)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构建词汇表\n",
    "import jieba\n",
    "import json\n",
    "'''\n",
    "def build_vocab(text_list):\n",
    "    vocab = {\"<unk>\": 0}  # 添加一个特殊的索引，用于表示未知词\n",
    "\n",
    "    # 遍历所有句子，构建词汇表\n",
    "    for text in text_list:\n",
    "        word_list = jieba.lcut(text)\n",
    "        for word in word_list:\n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab)  # 将每个词映射到唯一的整数索引\n",
    "\n",
    "    return vocab\n",
    "\n",
    "vocabulary = build_vocab(train_data['text'].tolist() + dev_data['text'].tolist())\n",
    "# 保存词汇表到 JSON 文件, 下次可以直接使用\n",
    "with open('vocabulary.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(vocabulary, f, ensure_ascii=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取 JSON 文件中的词汇表\n",
    "with open('vocabulary.json', 'r', encoding='utf-8') as f:\n",
    "    vocabulary = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转文本向量\n",
    "class TextVector(object):\n",
    "    def __init__(self, text_list, vocabulary):\n",
    "        self.text_list = text_list\n",
    "        self.vocabulary = vocabulary\n",
    "\n",
    "    def text2vector(self, max_len = 30):\n",
    "        all_indexed_sentences = []\n",
    "\n",
    "        # 遍历所有句子，将词汇映射为整数索引，并进行填充\n",
    "        for text in self.text_list:\n",
    "            word_list = jieba.lcut(text)\n",
    "            indexed_sentence = [self.vocabulary.get(word, self.vocabulary[\"<unk>\"]) for word in word_list]\n",
    "\n",
    "            # 填充句子至最大长度\n",
    "            padded_sentence = indexed_sentence + [0] * (max_len - len(indexed_sentence))\n",
    "            all_indexed_sentences.append(padded_sentence)\n",
    "\n",
    "        return all_indexed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_vector = TextVector(train_data['text'][:1024].tolist(), vocabulary)\n",
    "train_vectors = train_text_vector.text2vector()\n",
    "dev_text_vector = TextVector(dev_data['text'][:1024].tolist(), vocabulary)\n",
    "dev_vectors = dev_text_vector.text2vector()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T05:42:28.627563Z",
     "iopub.status.busy": "2022-07-20T05:42:28.626778Z",
     "iopub.status.idle": "2022-07-20T05:42:28.638724Z",
     "shell.execute_reply": "2022-07-20T05:42:28.638000Z",
     "shell.execute_reply.started": "2022-07-20T05:42:28.627517Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Tensor(shape=[30], dtype=int64, place=Place(gpu:0), stop_gradient=True,\n",
      "       [1, 2, 3, 4, 5, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0]), Tensor(shape=[1], dtype=int64, place=Place(gpu:0), stop_gradient=True,\n",
      "       [0]))\n",
      "(Tensor(shape=[30], dtype=int64, place=Place(gpu:0), stop_gradient=True,\n",
      "       [5532, 5416, 2429, 831 , 8997, 475 , 27229, 2101, 276 , 1058, 17183, 1857,\n",
      "        2054, 0   , 0   , 0   , 0   , 0   , 0   , 0   , 0   , 0   , 0   , 0   ,\n",
      "        0   , 0   , 0   , 0   , 0   , 0   ]), Tensor(shape=[1], dtype=int64, place=Place(gpu:0), stop_gradient=True,\n",
      "       [11]))\n"
     ]
    }
   ],
   "source": [
    "# 定义训练数据集\n",
    "class TrainData(Dataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text = paddle.to_tensor(train_vectors[index], dtype='int64')\n",
    "        label = paddle.to_tensor(train_data['label'].tolist()[index], dtype='int64')\n",
    "\n",
    "        return text, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(train_vectors)\n",
    "\n",
    "\n",
    "# 定义验证数据集\n",
    "class DevData(Dataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text = paddle.to_tensor(dev_vectors[index], dtype='int64')\n",
    "        label = paddle.to_tensor(dev_data['label'].tolist()[index], dtype='int64')\n",
    "\n",
    "        return text, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(dev_vectors)\n",
    "\n",
    "    \n",
    "train_dataset = TrainData()\n",
    "print(train_dataset.__getitem__(0))\n",
    "dev_dataset = DevData()\n",
    "print(dev_dataset.__getitem__(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3、准备网络模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Layer):\n",
    "    # vocab_size字典里面的个数\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, time_major=False)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "\n",
    "        x = self.embed(x)\n",
    "        out, (h, c) = self.lstm(x, h)\n",
    "        out = out.reshape([out.shape[0] * out.shape[1], out.shape[2]])\n",
    "        out = self.linear(out)\n",
    "\n",
    "        return out, (h, c)\n",
    "\n",
    "# 定义模型\n",
    "embed_size = 128 # embed_size: 词嵌入后的特征数；\n",
    "hidden_size = 1024 # hidden_size: lstm中隐层的节点数；\n",
    "num_layers = 1 # num_layers: lstm中的隐层数量；\n",
    "vocab_size = len(vocabulary) # 词汇数量\n",
    "maxlength = 30  # 新闻标题的最大长度 \n",
    "\n",
    "generator= Generator(vocab_size, embed_size, hidden_size, num_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "判别器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Layer):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, kernel_size, output_size, maxlength):\n",
    "        super().__init__()\n",
    "        self.embed=nn.Embedding(vocab_size,embedding_dim,padding_idx=0)\n",
    "        self.cnn=nn.Conv1D(embedding_dim, hidden_dim, kernel_size)\n",
    "        self.maxpool=nn.MaxPool1D(maxlength-kernel_size+1)\n",
    "        self.dense=nn.Sequential(nn.Dropout(0.3), nn.Linear(hidden_dim, output_size))\n",
    "    def forward(self,x):\n",
    "        embed_x=self.embed(x)\n",
    "        cnn_x = self.cnn(embed_x.transpose((0, 2, 1)))\n",
    "        pool_x=self.maxpool(cnn_x)\n",
    "        out=self.dense(pool_x.squeeze(-1))\n",
    "        out=nn.functional.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# 定义模型\n",
    "vocab_size # 词汇数量\n",
    "embedding_dim = 1024 # 词嵌入维度\n",
    "hidden_dim = 128 # 隐藏层维度，也就是CNN网络层卷积核的个数\n",
    "kernel_size = 3 # 卷积核大小\n",
    "output_size = 1  # 分类的类别数\n",
    "\n",
    "discriminator = Discriminator(vocab_size, embedding_dim, hidden_dim, kernel_size, output_size, maxlength)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (embed): Embedding(251988, 128, sparse=False)\n",
      "  (lstm): LSTM(128, 1024\n",
      "    (0): RNN(\n",
      "      (cell): LSTMCell(128, 1024)\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=1024, out_features=251988, dtype=float32)\n",
      ")\n",
      "Discriminator(\n",
      "  (embed): Embedding(251988, 1024, padding_idx=0, sparse=False)\n",
      "  (cnn): Conv1D(1024, 128, kernel_size=[3], data_format=NCL)\n",
      "  (maxpool): MaxPool1D(kernel_size=28, stride=None, padding=0)\n",
      "  (dense): Sequential(\n",
      "    (0): Dropout(p=0.3, axis=None, mode=upscale_in_train)\n",
      "    (1): Linear(in_features=128, out_features=1, dtype=float32)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 输出模型结构\n",
    "print(generator)\n",
    "print(discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "载入预训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrain_model_path = './pretrained_models/cnn/final.pdparams'# 'pretrained_models/ResNet50_pretrained'\n",
    "\n",
    "# 加载预训练模型参数\n",
    "# model.set_state_dict(paddle.load(pretrain_model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4、模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "环境设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU.\n"
     ]
    }
   ],
   "source": [
    "# 可视化\n",
    "from visualdl import LogWriter\n",
    "logwriter = LogWriter(logdir='./visualdl/gan')\n",
    "\n",
    "# 设置GPU环境，如果没有GPU则设置为CPU\n",
    "if paddle.is_compiled_with_cuda() and paddle.get_device() != 'cpu':\n",
    "    paddle.set_device('gpu:0')\n",
    "    print(\"Using GPU.\")\n",
    "else:\n",
    "    paddle.set_device('cpu')\n",
    "    print(\"Using CPU.\")\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 512\n",
    "\n",
    "# embed_size: 词嵌入后的特征数；\n",
    "embed_size = 128\n",
    "# hidden_size: lstm中隐层的节点数；\n",
    "hidden_size = 1024\n",
    "# num_layers: lstm中的隐层数量；\n",
    "num_layers = 1\n",
    "# seq_length: 获取的序列长度\n",
    "seq_length = 30\n",
    "# learning_rate: 模型的学习率\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "辅助函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_list(length1, length2):\n",
    "    random_list = [random.randint(0, length2 - 1) for _ in range(length1)]\n",
    "    return random_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_words(model, states, num_samples=30):\n",
    "    # prob对应模型中的outputs，是输入变量经过语言模型得到的输出值，相当于此时每个单词的概率分布\n",
    "    # 生成概率分布，假设每个词的概率相等\n",
    "    prob = paddle.ones([vocab_size], dtype='float32') / vocab_size\n",
    "    # 生成随机样本\n",
    "    samples = paddle.multinomial(prob, num_samples=batch_size)\n",
    "    # 将结果变形成 [batch_size, 1] 的张量\n",
    "    input = paddle.unsqueeze(samples, axis=1) \n",
    "\n",
    "    words = paddle.empty([batch_size, 0], dtype='int64')  # 创建一个空张量，用于存储结果\n",
    "    for i in range(num_samples):\n",
    "        output, states = model(input, states)\n",
    "        # prob是对上一步得到的output进行指数化，加强高概率结果的权重；\n",
    "        prob = output.exp()\n",
    "        # word_id，通过torch_multinomial，以prob为权重，对结果进行加权抽样，样本数为1(即num_samples)\n",
    "        word_id = paddle.multinomial(prob, num_samples=1)\n",
    "        words = paddle.concat([words, word_id], axis=1)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 定义数据迭代器\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "valid_dataloader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.BCELoss()\n",
    "optimizer_G = paddle.optimizer.Adam(parameters=generator.parameters(), learning_rate=learning_rate)\n",
    "optimizer_D = paddle.optimizer.Adam(parameters=discriminator.parameters(), learning_rate=learning_rate)\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    # states是参数矩阵的初始化，相当于对LSTMmodel类里的(h, c)的初始化；\n",
    "    states = (paddle.zeros(shape=[num_layers, batch_size, hidden_size],dtype='float32'),\n",
    "            paddle.zeros(shape=[num_layers, batch_size, hidden_size],dtype='float32'))\n",
    " \n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        # train discriminator\n",
    "        # 使用真实数据集\n",
    "        real_data = data[0]\n",
    "        real_labels = paddle.ones((batch_size, 1)) # [256, 1]\n",
    "        optimizer_G.clear_grad()\n",
    "        real_output = discriminator(real_data) # [batch]\n",
    "        loss_real = criterion(real_output, real_labels) # [1]\n",
    "\n",
    "        # 使用生成数据集\n",
    "        states = [state.detach() for state in states]\n",
    "        fake_data = generate_words(generator, states)\n",
    "        fake_labels = paddle.zeros((batch_size, 1))\n",
    "        fake_output = discriminator(fake_data)\n",
    "        loss_fake = criterion(fake_output, fake_labels)\n",
    "        \n",
    "        loss_D = loss_real + loss_fake\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # 训练生成器\n",
    "        optimizer_G.clear_grad()\n",
    "        fake_data = generate_words(generator, states)\n",
    "        fake_output = discriminator(fake_data)\n",
    "\n",
    "        loss_G = criterion(fake_output, real_labels)\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        if i%50==0:\n",
    "                print('[epoch/Epoch {}/{} iter/Iter {}/{}] lossD {:.4f}, lossG {:.4f}'.format(epoch, epochs-1, i+1, len(train_dataloader), loss_D.item(), loss_G.item()))\n",
    "                logwriter.add_scalar(\"generator_loss\", value=loss_G.item(), step=i+epoch*(batch_size))\n",
    "                logwriter.add_scalar(\"discriminator_loss\", value=loss_D.item(), step=i+epoch*(batch_size))\n",
    "            \n",
    "        if epoch%2 == 0:\n",
    "            paddle.save(generator.state_dict(), os.path.join('model', 'gan', 'generator{}.pdmodel'.format(str(epoch).zfill(4))))\n",
    "            paddle.save(discriminator.state_dict(), os.path.join('model', 'gan', 'discriminator{}.pdmodel'.format(str(epoch).zfill(4))))\n",
    "        \n",
    "    \n",
    "    paddle.save(generator.state_dict(), os.path.join('model', 'gan', 'generator.pdmodel'))\n",
    "    paddle.save(discriminator.state_dict(), os.path.join('model', 'gan', 'discriminator.pdmodel'))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5、模型测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "reversed_vocabulary = list(vocabulary.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-20T06:02:27.749161Z",
     "iopub.status.busy": "2022-07-20T06:02:27.748067Z",
     "iopub.status.idle": "2022-07-20T06:02:34.667377Z",
     "shell.execute_reply": "2022-07-20T06:02:34.666235Z",
     "shell.execute_reply.started": "2022-07-20T06:02:27.749119Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "万人迷MedAssets向利丰欢愉i9088Cat可转可赔相映成辉剪系\n"
     ]
    }
   ],
   "source": [
    "generator = Generator(vocab_size, embed_size, hidden_size, num_layers)\n",
    "# 模型载入\n",
    "gen_path_dict = paddle.load('./model/gan/generator.pdmodel')\n",
    "generator.set_state_dict(gen_path_dict)\n",
    "\n",
    "states = (paddle.zeros(shape=[num_layers, 1, hidden_size],dtype='float32'),\n",
    "            paddle.zeros(shape=[num_layers, 1, hidden_size],dtype='float32'))\n",
    "\n",
    "states = [state.detach() for state in states]\n",
    "\n",
    "title=''\n",
    "\n",
    "prob = paddle.ones([vocab_size], dtype='float32') / vocab_size\n",
    "input = paddle.multinomial(prob, num_samples=1).unsqueeze(1)\n",
    "for i in range(10):\n",
    "    output, states = generator(input, states)\n",
    "    prob = output.exp()\n",
    "    word_id = paddle.multinomial(prob, num_samples=1)\n",
    "    # 从字典映射表Dictionary里，找到当前索引(即word_id)对应的单词；\n",
    "    word = reversed_vocabulary[word_id.item()]\n",
    "    title += word\n",
    "\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "修改判别器输出，将其调整为分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/fluid/dygraph/layers.py:1517: UserWarning: Skip loading for dense.1.weight. dense.1.weight receives a shape [128, 1], but the expected shape is [128, 14].\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n",
      "/root/miniconda3/envs/paddle/lib/python3.8/site-packages/paddle/fluid/dygraph/layers.py:1517: UserWarning: Skip loading for dense.1.bias. dense.1.bias receives a shape [1], but the expected shape is [14].\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n"
     ]
    }
   ],
   "source": [
    "classifier = Discriminator(vocab_size, embedding_dim, hidden_dim, kernel_size, 14, maxlength)\n",
    "# 模型载入\n",
    "dis_path_dict = paddle.load('./model/gan/discriminator.pdmodel')\n",
    "classifier.set_state_dict(dis_path_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss value printed in the log is the current step, and the metric is the average value of previous steps.\n",
      "Epoch 1/10\n",
      "step 40/40 [==============================] - loss: 2.3949 - acc: 0.2764 - 4s/step           \n",
      "save checkpoint at /home/.kun/.study/ANN/final/title_classify/model/gan/0\n",
      "Eval begin...\n",
      "step 4/4 [==============================] - loss: 2.4494 - acc: 0.2549 - 257ms/step\n",
      "Eval samples: 1024\n",
      "Epoch 2/10\n",
      "step 40/40 [==============================] - loss: 2.3650 - acc: 0.2894 - 4s/step           \n",
      "Eval begin...\n",
      "step 4/4 [==============================] - loss: 2.4411 - acc: 0.2598 - 267ms/step\n",
      "Eval samples: 1024\n",
      "Epoch 3/10\n",
      "step 40/40 [==============================] - loss: 2.3352 - acc: 0.2921 - 4s/step           \n",
      "save checkpoint at /home/.kun/.study/ANN/final/title_classify/model/gan/2\n",
      "Eval begin...\n",
      "step 4/4 [==============================] - loss: 2.4219 - acc: 0.2773 - 260ms/step\n",
      "Eval samples: 1024\n",
      "Epoch 4/10\n",
      "step 40/40 [==============================] - loss: 2.3509 - acc: 0.3087 - 3s/step           \n",
      "Eval begin...\n",
      "step 4/4 [==============================] - loss: 2.3961 - acc: 0.2969 - 225ms/step\n",
      "Eval samples: 1024\n",
      "Epoch 5/10\n",
      "step 40/40 [==============================] - loss: 2.3184 - acc: 0.3341 - 3s/step           \n",
      "save checkpoint at /home/.kun/.study/ANN/final/title_classify/model/gan/4\n",
      "Eval begin...\n",
      "step 4/4 [==============================] - loss: 2.3653 - acc: 0.3311 - 258ms/step\n",
      "Eval samples: 1024\n",
      "Epoch 6/10\n",
      "step 40/40 [==============================] - loss: 2.2684 - acc: 0.3653 - 3s/step           \n",
      "Eval begin...\n",
      "step 4/4 [==============================] - loss: 2.3701 - acc: 0.3760 - 272ms/step\n",
      "Eval samples: 1024\n",
      "Epoch 7/10\n",
      "step 40/40 [==============================] - loss: 2.2626 - acc: 0.3928 - 4s/step           \n",
      "save checkpoint at /home/.kun/.study/ANN/final/title_classify/model/gan/6\n",
      "Eval begin...\n",
      "step 4/4 [==============================] - loss: 2.3312 - acc: 0.4189 - 273ms/step\n",
      "Eval samples: 1024\n",
      "Epoch 8/10\n",
      "step 40/40 [==============================] - loss: 2.2071 - acc: 0.4244 - 3s/step           \n",
      "Eval begin...\n",
      "step 4/4 [==============================] - loss: 2.3009 - acc: 0.4482 - 262ms/step\n",
      "Eval samples: 1024\n",
      "Epoch 9/10\n",
      "step 40/40 [==============================] - loss: 2.1637 - acc: 0.4562 - 3s/step           \n",
      "save checkpoint at /home/.kun/.study/ANN/final/title_classify/model/gan/8\n",
      "Eval begin...\n",
      "step 4/4 [==============================] - loss: 2.2896 - acc: 0.4707 - 265ms/step\n",
      "Eval samples: 1024\n",
      "Epoch 10/10\n",
      "step 40/40 [==============================] - loss: 2.1604 - acc: 0.4806 - 4s/step           \n",
      "Eval begin...\n",
      "step 4/4 [==============================] - loss: 2.2938 - acc: 0.4834 - 250ms/step\n",
      "Eval samples: 1024\n",
      "save checkpoint at /home/.kun/.study/ANN/final/title_classify/model/gan/final\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 256\n",
    "\n",
    "# 定义数据迭代器\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "valid_dataloader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "# 定义优化器\n",
    "opt = paddle.optimizer.Adam(learning_rate=1e-4, parameters=classifier.parameters(), weight_decay=paddle.regularizer.L2Decay(1e-4))\n",
    "\n",
    "# 定义损失函数\n",
    "loss_fn = paddle.nn.CrossEntropyLoss()\n",
    "\n",
    "# 用于测量准确率的评价指标对象\n",
    "metric =  paddle.metric.Accuracy()\n",
    "\n",
    "# 定义 EarlyStopping 回调函数\n",
    "callback = paddle.callbacks.EarlyStopping(monitor='acc', patience=5, mode='max', verbose=1)\n",
    "# 定义 ModelCheckpoint 回调函数\n",
    "checkpoint_callback = paddle.callbacks.ModelCheckpoint(save_dir='./model/gan', save_freq=2)\n",
    "# 设置 visualdl 路径\n",
    "log_dir = './visualdl/gan'\n",
    "visual_callback = paddle.callbacks.VisualDL(log_dir=log_dir)\n",
    "\n",
    "# 使用高层API进行训练\n",
    "model = paddle.Model(classifier) # 用 Model 封装\n",
    "# 模型配置\n",
    "model.prepare(opt, loss_fn, metric)\n",
    "# 模型训练\n",
    "model.fit(train_dataloader,\n",
    "          valid_dataloader,\n",
    "          epochs=epochs,\n",
    "          batch_size=batch_size,\n",
    "          verbose=1,\n",
    "          callbacks= [callback, checkpoint_callback, visual_callback])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paddle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
